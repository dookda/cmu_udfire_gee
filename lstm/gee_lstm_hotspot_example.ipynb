{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hotspot Prediction with Google Earth Engine (GEE) and LSTM\n",
        "\n",
        "**Scope:** End-to-end template in Python for:\n",
        "1) Authenticating & reading data from GEE\n",
        "2) Building weekly features for a province-level AOI (Nan, Thailand)\n",
        "3) Creating a supervised dataset\n",
        "4) Training an LSTM to forecast hotspots\n",
        "5) Evaluating and forecasting for 2025\n",
        "\n",
        "> **Tip:** Run this notebook in Google Colab for smooth GEE auth and Drive access. On first run, you'll be prompted to authorize Earth Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q earthengine-api geemap pandas numpy matplotlib scikit-learn tensorflow\n",
        "import ee, json, datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Authenticate & Initialize Earth Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    ee.Initialize()\n",
        "except Exception:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "print('GEE initialized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Define area of interest (AOI): Nan Province, Thailand\n",
        "\n",
        "We try FAO GAUL level-1 for provinces. If GAUL is unavailable in your project, switch to GADM or use your own shapefile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_nan_province():\n",
        "    # Option A: FAO GAUL simplified level 1\n",
        "    try:\n",
        "        gaul1 = ee.FeatureCollection('FAO/GAUL/2015/level1')\n",
        "        nan_fc = gaul1.filter(ee.Filter.And(\n",
        "            ee.Filter.eq('ADM0_NAME', 'Thailand'),\n",
        "            ee.Filter.eq('ADM1_NAME', 'Nan')\n",
        "        ))\n",
        "        geom = ee.Feature(nan_fc.first()).geometry()\n",
        "        return geom\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Option B: GAUL simplified 500m level 1\n",
        "    gaul1s = ee.FeatureCollection('FAO/GAUL_SIMPLIFIED_500m/2015/level1')\n",
        "    nan_fc = gaul1s.filter(ee.Filter.And(\n",
        "        ee.Filter.eq('ADM0_NAME', 'Thailand'),\n",
        "        ee.Filter.eq('ADM1_NAME', 'Nan')\n",
        "    ))\n",
        "    return ee.Feature(nan_fc.first()).geometry()\n",
        "\n",
        "AOI = get_nan_province()\n",
        "AOI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Configure time windows\n",
        "\n",
        "We aggregate weekly features for 2020â€“2024 and predict 2025."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_start = ee.Date('2020-01-01')\n",
        "train_end   = ee.Date('2024-12-31')\n",
        "test_start  = ee.Date('2025-01-01')\n",
        "test_end    = ee.Date('2025-12-31')\n",
        "\n",
        "WEEK_MILLIS = 7 * 24 * 60 * 60 * 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Data sources and feature engineering\n",
        "\n",
        "Targets (hotspots):\n",
        "- **VIIRS Fire Mask** daily product (example): `NOAA/VIIRS/001/VNP14A1` (band `FireMask`). We count pixels indicating active fire per week.\n",
        "\n",
        "Predictors:\n",
        "- **NDVI/NDMI** from Landsat-8/9 SR or MODIS (MOD13Q1). Here we use MODIS 16-day NDVI/NDMI proxies and resample.\n",
        "- **Topography**: SRTM slope.\n",
        "- **Climate**: ERA5-Land daily (temperature, total precipitation). We weekly-aggregate.\n",
        "\n",
        "ðŸ‘‰ *You may switch to your preferred datasets; the logic stays the same.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Helper: weekly date list ---\n",
        "def weekly_dates(start, end):\n",
        "    start = ee.Date(start)\n",
        "    end = ee.Date(end)\n",
        "    n_weeks = end.difference(start, 'week').toInt()\n",
        "    def make_week(i):\n",
        "        s = start.advance(i, 'week')\n",
        "        e = s.advance(1, 'week')\n",
        "        return ee.Feature(None, {'start': s, 'end': e})\n",
        "    return ee.FeatureCollection(ee.List.sequence(0, n_weeks.subtract(1)).map(make_week))\n",
        "\n",
        "# --- Target: VIIRS VNP14A1 FireMask (daily) -> weekly fire pixel count ---\n",
        "viirs = ee.ImageCollection('NOAA/VIIRS/001/VNP14A1')\n",
        "\n",
        "def weekly_fire_count(start, end, geom):\n",
        "    # FireMask > 7 typically indicates detected fire; adjust as needed\n",
        "    col = viirs.filterDate(start, end).select('FireMask')\n",
        "    # Convert to binary fire image per day\n",
        "    def to_fire(img):\n",
        "        fire = img.gte(7).selfMask()\n",
        "        return fire.rename('fire')\n",
        "    daily = col.map(to_fire)\n",
        "    # Sum over the week (count of fire pixels appearances)\n",
        "    summed = daily.sum()\n",
        "    # Pixel area to scale counts if desired (optional)\n",
        "    count = summed.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(),\n",
        "        geometry=geom, scale=500, maxPixels=1e13\n",
        "    )\n",
        "    return ee.Number(count.get('fire')).unbound()\n",
        "\n",
        "# --- Predictors ---\n",
        "# NDVI from MODIS (MOD13Q1.061) 16-day, 250m\n",
        "modis_ndvi = ee.ImageCollection('MODIS/061/MOD13Q1').select('NDVI')\n",
        "\n",
        "def weekly_ndvi_mean(start, end, geom):\n",
        "    col = modis_ndvi.filterDate(start, end)\n",
        "    mean = col.mean().multiply(0.0001)  # scale factor\n",
        "    val = mean.reduceRegion(ee.Reducer.mean(), geom, scale=500, maxPixels=1e13)\n",
        "    return ee.Number(val.get('NDVI'))\n",
        "\n",
        "# NDMI proxy from MODIS NIR/SWIR (using MOD09A1 SR 500m 8-day)\n",
        "modis_sr = ee.ImageCollection('MODIS/061/MOD09A1').select(['sur_refl_b02','sur_refl_b06'])\n",
        "def weekly_ndmi_mean(start, end, geom):\n",
        "    col = modis_sr.filterDate(start, end)\n",
        "    def ndmi(img):\n",
        "        nir = img.select('sur_refl_b02').multiply(0.0001)\n",
        "        swir = img.select('sur_refl_b06').multiply(0.0001)\n",
        "        n = nir.subtract(swir).divide(nir.add(swir)).rename('NDMI')\n",
        "        return n\n",
        "    ndmi_col = col.map(ndmi)\n",
        "    mean = ndmi_col.mean()\n",
        "    val = mean.reduceRegion(ee.Reducer.mean(), geom, scale=500, maxPixels=1e13)\n",
        "    return ee.Number(val.get('NDMI'))\n",
        "\n",
        "# Slope from SRTM\n",
        "srtm = ee.Image('USGS/SRTMGL1_003')\n",
        "slope = ee.Terrain.slope(srtm)\n",
        "def slope_mean(geom):\n",
        "    val = slope.reduceRegion(ee.Reducer.mean(), geom, scale=500, maxPixels=1e13)\n",
        "    return ee.Number(val.get('slope'))\n",
        "\n",
        "# ERA5-Land: daily temperature (K) and total precipitation (m)\n",
        "era5 = ee.ImageCollection('ECMWF/ERA5_LAND/DAILY')\n",
        "def weekly_era5(start, end, geom):\n",
        "    col = era5.filterDate(start, end)\n",
        "    t2m = col.select('temperature_2m').mean()\n",
        "    tp = col.select('total_precipitation').sum()\n",
        "    t_mean = t2m.reduceRegion(ee.Reducer.mean(), geom, scale=1000, maxPixels=1e13).get('temperature_2m')\n",
        "    p_sum  = tp.reduceRegion(ee.Reducer.mean(), geom, scale=1000, maxPixels=1e13).get('total_precipitation')\n",
        "    return ee.Dictionary({'t2m_mean': t_mean, 'tp_sum': p_sum})\n",
        "\n",
        "# Precompute static slope\n",
        "SLOPE_MEAN = slope_mean(AOI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build weekly feature table\n",
        "For each week we compute:\n",
        "- `y_fire`: count of active-fire pixels (target)\n",
        "- `ndvi_mean`, `ndmi_mean`\n",
        "- `t2m_mean` (K), `tp_sum` (m)\n",
        "- `slope_mean` (static)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_weekly_df(start, end, geom):\n",
        "    weeks = weekly_dates(start, end)\n",
        "    def per_week(f):\n",
        "        s = ee.Date(f.get('start'))\n",
        "        e = ee.Date(f.get('end'))\n",
        "        y = weekly_fire_count(s, e, geom)\n",
        "        ndvi = weekly_ndvi_mean(s, e, geom)\n",
        "        ndmi = weekly_ndmi_mean(s, e, geom)\n",
        "        clim = weekly_era5(s, e, geom)\n",
        "        return ee.Feature(None, {\n",
        "            'start': s.format('YYYY-MM-dd'),\n",
        "            'end': e.format('YYYY-MM-dd'),\n",
        "            'y_fire': y,\n",
        "            'ndvi_mean': ndvi,\n",
        "            'ndmi_mean': ndmi,\n",
        "            't2m_mean': clim.get('t2m_mean'),\n",
        "            'tp_sum': clim.get('tp_sum'),\n",
        "            'slope_mean': SLOPE_MEAN\n",
        "        })\n",
        "    fc = weeks.map(per_week)\n",
        "    # Download to client as a list of dicts\n",
        "    rows = fc.getInfo()['features']\n",
        "    recs = [r['properties'] for r in rows]\n",
        "    df = pd.DataFrame(recs)\n",
        "    df['start'] = pd.to_datetime(df['start'])\n",
        "    df['end'] = pd.to_datetime(df['end'])\n",
        "    df = df.sort_values('start').reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "df_train = build_weekly_df(train_start, train_end, AOI)\n",
        "df_test  = build_weekly_df(test_start, test_end, AOI)\n",
        "df_train.head(), df_train.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Prepare sequences for LSTM\n",
        "\n",
        "- We use a sliding window of `seq_len` weeks to predict next-week hotspots.\n",
        "- Feature scaling is applied to predictors and the target.\n",
        "\n",
        "You can add more lags/exogenous features as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['ndvi_mean','ndmi_mean','t2m_mean','tp_sum','slope_mean','y_fire']\n",
        "seq_len = 8  # 8-week lookback\n",
        "\n",
        "def make_sequences(df, seq_len, feature_cols, target_col='y_fire'):\n",
        "    values = df[feature_cols].astype(float).values\n",
        "    scaler = MinMaxScaler()\n",
        "    values_scaled = scaler.fit_transform(values)\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(values_scaled) - seq_len):\n",
        "        Xs.append(values_scaled[i:i+seq_len, :])\n",
        "        ys.append(values_scaled[i+seq_len, feature_cols.index(target_col)])\n",
        "    X = np.array(Xs)\n",
        "    y = np.array(ys)\n",
        "    return X, y, scaler\n",
        "\n",
        "X_train, y_train, scaler = make_sequences(df_train, seq_len, features)\n",
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Define & train the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "n_timesteps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(n_timesteps, n_features)),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse')\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='val')\n",
        "plt.legend(); plt.title('Loss'); plt.xlabel('epoch'); plt.ylabel('MSE'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Evaluate on 2024 holdout (last few weeks) and Forecast 2025\n",
        "\n",
        "We build rolling sequences across the boundary from 2024 into 2025 and inverse-transform only the target dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_with_existing(df, scaler, feature_cols):\n",
        "    vals = df[feature_cols].astype(float).values\n",
        "    return scaler.transform(vals)\n",
        "\n",
        "def build_sequences_from_scaled(scaled_vals, seq_len, target_index):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(scaled_vals) - seq_len):\n",
        "        Xs.append(scaled_vals[i:i+seq_len, :])\n",
        "        ys.append(scaled_vals[i+seq_len, target_index])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "TARGET_IDX = features.index('y_fire')\n",
        "\n",
        "# Evaluate on tail of train (e.g., last 20% already handled by val). Here we proceed to forecasting on 2025.\n",
        "# To forecast next-week iteratively in 2025, we combine train+test features and slide a window.\n",
        "df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
        "scaled_all = scale_with_existing(df_all, scaler, features)\n",
        "X_all, y_all = build_sequences_from_scaled(scaled_all, seq_len, TARGET_IDX)\n",
        "\n",
        "# Predictions for all available windows; extract the windows that end in 2025\n",
        "pred_scaled = model.predict(X_all)\n",
        "\n",
        "# Inverse transform: construct an array with same feature width, put preds in target column\n",
        "def inverse_target_only(pred_scaled, scaler, target_index):\n",
        "    dummy = np.zeros((len(pred_scaled), len(features)))\n",
        "    dummy[:, target_index] = pred_scaled.flatten()\n",
        "    inv = scaler.inverse_transform(dummy)\n",
        "    return inv[:, target_index]\n",
        "\n",
        "pred_y = inverse_target_only(pred_scaled, scaler, TARGET_IDX)\n",
        "\n",
        "# Align predictions to dates: prediction at position i corresponds to week index i+seq_len\n",
        "dates = df_all['end'].values\n",
        "pred_dates = dates[seq_len:]\n",
        "\n",
        "pred_df = pd.DataFrame({'date': pred_dates, 'y_hat': pred_y})\n",
        "\n",
        "# Merge actuals (if available)\n",
        "actual = df_all.iloc[seq_len:][['end','y_fire']].reset_index(drop=True).rename(columns={'end':'date'})\n",
        "eval_df = pred_df.merge(actual, on='date', how='left')\n",
        "\n",
        "# Split to 2025 subset\n",
        "eval_df['date'] = pd.to_datetime(eval_df['date'])\n",
        "df_2025 = eval_df[eval_df['date'].dt.year == 2025].copy()\n",
        "df_2025.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics (if you have actual 2025 data)\n",
        "If actual 2025 targets exist (once the period passes), compute metrics; otherwise, visualize forecasts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "has_actuals = df_2025['y_fire'].notna().any()\n",
        "if has_actuals:\n",
        "    mse = mean_squared_error(df_2025['y_fire'], df_2025['y_hat'])\n",
        "    mae = mean_absolute_error(df_2025['y_fire'], df_2025['y_hat'])\n",
        "    r2  = r2_score(df_2025['y_fire'], df_2025['y_hat'])\n",
        "    print('MSE:', mse, 'MAE:', mae, 'R2:', r2)\n",
        "else:\n",
        "    print('No 2025 actuals available for metrics. Showing forecast only.')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(df_2025['date'], df_2025['y_hat'], label='Forecast (y_hat)')\n",
        "if has_actuals:\n",
        "    plt.plot(df_2025['date'], df_2025['y_fire'], label='Actual (y)')\n",
        "plt.legend(); plt.title('Weekly Hotspot Forecast â€“ 2025 (Nan, TH)')\n",
        "plt.xlabel('date'); plt.ylabel('weekly hotspot count')\n",
        "plt.xticks(rotation=45); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Next steps & options\n",
        "- Replace MODIS with Landsat 8/9 SR (NDVI, NDMI from NIR/SWIR) for higher spatial detail.\n",
        "- Add drought indices (SPEI), relative humidity, wind speed from ERA5.\n",
        "- Use **ConvLSTM** or Temporal Convolutional Networks if you move to gridded predictors instead of province averages.\n",
        "- Export an example dataset to CSV for audit:\n",
        "```python\n",
        "df_train.to_csv('nan_weekly_train.csv', index=False)\n",
        "df_test.to_csv('nan_weekly_test.csv', index=False)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}