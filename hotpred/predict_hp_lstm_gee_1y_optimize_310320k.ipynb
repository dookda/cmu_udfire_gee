{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dookda/cmu_udfire_gee/blob/main/hotpred/predict_hp_lstm_gee_1y_optimize_310320k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280c6ecc",
      "metadata": {
        "id": "280c6ecc"
      },
      "outputs": [],
      "source": [
        "# เริ่มต้นใช้งาน GEE\n",
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import calendar\n",
        "\n",
        "ee.Authenticate()\n",
        "try:\n",
        "    ee.Initialize(project=\"ee-sakda-451407\")\n",
        "except Exception as e:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project=\"ee-sakda-451407\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d909c7",
      "metadata": {
        "id": "69d909c7"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "# กำหนดพื้นที่ศึกษา เชียงใหม่\n",
        "# siteName = 'Mae Hong Son'\n",
        "siteName = 'Chiang Mai'\n",
        "# siteName = 'Nan'\n",
        "# siteName = 'Uttaradit'\n",
        "study_area = ee.FeatureCollection(\"FAO/GAUL/2015/level1\").filter(ee.Filter.eq('ADM0_NAME', 'Thailand')).filter(ee.Filter.eq('ADM1_NAME', siteName))\n",
        "\n",
        "# config figure height\n",
        "f = folium.Figure(height=300)\n",
        "\n",
        "# add map to figure\n",
        "m = folium.Map(location=[18.9, 99.0], zoom_start=7).add_to(f)\n",
        "\n",
        "# add study area to map\n",
        "folium.GeoJson(study_area.getInfo()).add_to(m)\n",
        "\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b34e3c1",
      "metadata": {
        "id": "5b34e3c1"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Model Evaluation with Baseline Models and Prediction Intervals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "    from statsmodels.tsa.stattools import adfuller\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "    print(\"Warning: statsmodels ไม่พร้อมใช้งาน บาง baseline models จะไม่ทำงาน\")\n",
        "\n",
        "try:\n",
        "    import pmdarima as pm\n",
        "    PMDARIMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PMDARIMA_AVAILABLE = False\n",
        "    print(\"Warning: pmdarima ไม่พร้อมใช้งาน Auto-ARIMA จะไม่ทำงาน\")\n",
        "\n",
        "def get_monthly_hotspots(start_date, end_date, study_area, temp_min=310, temp_max=320, all_hotspots=False):\n",
        "    \"\"\"\n",
        "    ดึงข้อมูล hotspot รายเดือนด้วยการกรองอุณหภูมิ T21 ในช่วงที่กำหนด\n",
        "\n",
        "    Parameters:\n",
        "    - start_date, end_date: ช่วงวันที่\n",
        "    - study_area: พื้นที่ศึกษา\n",
        "    - temp_min: อุณหภูมิต่ำสุด (Kelvin) - default 310\n",
        "    - temp_max: อุณหภูมิสูงสุด (Kelvin) - default 320\n",
        "    - all_hotspots: ถ้า True ใช้เกณฑ์ T21 > 0 แทนการกรอง min-max\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # เรียกชุดข้อมูล FIRMS\n",
        "        firms = ee.ImageCollection('FIRMS') \\\n",
        "            .filterDate(start_date, end_date) \\\n",
        "            .filterBounds(study_area)\n",
        "\n",
        "        # ตรวจสอบว่ามีข้อมูลหรือไม่\n",
        "        size = firms.size().getInfo()\n",
        "        print(f\"จำนวนภาพ FIRMS ดิบ: {size}\")\n",
        "        if size == 0:\n",
        "            print(\"ไม่มีข้อมูล FIRMS ในพื้นที่ศึกษา\")\n",
        "            return None\n",
        "\n",
        "        if all_hotspots:\n",
        "            print(\"ใช้เกณฑ์: ทุกจุดความร้อน (T21 > 0)\")\n",
        "        else:\n",
        "            print(f\"ใช้เกณฑ์อุณหภูมิ: {temp_min}K ≤ T21 < {temp_max}K\")\n",
        "\n",
        "        # Apply temperature filter for T21\n",
        "        def convert_to_filtered_presence(image):\n",
        "            t21 = image.select('T21')\n",
        "            if all_hotspots:\n",
        "                hotspot_presence = t21.gt(0).rename('hotspot_presence')\n",
        "            else:\n",
        "                hotspot_presence = t21.gte(temp_min).And(t21.lt(temp_max)).rename('hotspot_presence')\n",
        "            return image.addBands(hotspot_presence).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "        firms_with_presence = firms.map(convert_to_filtered_presence)\n",
        "\n",
        "        # รวม hotspot เป็นรายเดือน\n",
        "        def create_monthly_hotspot_composite(year_month):\n",
        "            year = ee.Number(year_month).divide(100).floor().int()\n",
        "            month = ee.Number(year_month).mod(100).int()\n",
        "            start = ee.Date.fromYMD(year, month, 1)\n",
        "            end = start.advance(1, 'month')\n",
        "\n",
        "            monthly_collection = firms_with_presence.filterDate(start, end)\n",
        "            count = monthly_collection.size()\n",
        "\n",
        "            def process_with_data():\n",
        "                monthly_sum = monthly_collection.select('hotspot_presence').sum()\n",
        "                reduction = monthly_sum.reduceRegion(\n",
        "                    reducer=ee.Reducer.sum(),\n",
        "                    geometry=study_area,\n",
        "                    scale=1000,\n",
        "                    maxPixels=1e9\n",
        "                )\n",
        "                hotspot_count = ee.Number(reduction.get('hotspot_presence'))\n",
        "                hotspot_count = ee.Algorithms.If(hotspot_count, hotspot_count, ee.Number(0))\n",
        "\n",
        "                return ee.Image.constant(hotspot_count).toFloat().rename('hotspot_count') \\\n",
        "                    .set('system:time_start', start.millis()) \\\n",
        "                    .set('system:index', start.format('YYYY_MM')) \\\n",
        "                    .set('year', year) \\\n",
        "                    .set('month', month) \\\n",
        "                    .set('temp_filter_min', temp_min) \\\n",
        "                    .set('temp_filter_max', temp_max) \\\n",
        "                    .set('hotspot_count_prop', hotspot_count)\n",
        "\n",
        "            def process_without_data():\n",
        "                return ee.Image.constant(0).toFloat().rename('hotspot_count') \\\n",
        "                    .set('system:time_start', start.millis()) \\\n",
        "                    .set('system:index', start.format('YYYY_MM')) \\\n",
        "                    .set('year', year) \\\n",
        "                    .set('month', month) \\\n",
        "                    .set('temp_filter_min', temp_min) \\\n",
        "                    .set('temp_filter_max', temp_max) \\\n",
        "                    .set('hotspot_count_prop', ee.Number(0))\n",
        "\n",
        "            return ee.Algorithms.If(count.gt(0), process_with_data(), process_without_data())\n",
        "\n",
        "        # สร้างรายการปี-เดือน\n",
        "        start_year = int(start_date.split('-')[0])\n",
        "        end_year = int(end_date.split('-')[0])\n",
        "        year_months = []\n",
        "        for year in range(start_year, end_year + 1):\n",
        "            start_month = 1 if year > start_year else int(start_date.split('-')[1])\n",
        "            end_month = 12 if year < end_year else int(end_date.split('-')[1])\n",
        "            for month in range(start_month, end_month + 1):\n",
        "                year_months.append(year * 100 + month)\n",
        "\n",
        "        year_months_ee = ee.List(year_months)\n",
        "        monthly_images = year_months_ee.map(create_monthly_hotspot_composite)\n",
        "        monthly_composites = ee.ImageCollection.fromImages(monthly_images)\n",
        "\n",
        "        total_months = monthly_composites.size().getInfo()\n",
        "        print(f\"จำนวนเดือนทั้งหมด: {total_months}\")\n",
        "\n",
        "        return monthly_composites.select(['hotspot_count'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting monthly hotspots: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Updated create_monthly_dataset function to use separate NDVI and hotspot data\n",
        "def create_monthly_dataset_with_filtered_hotspots(ndvi_collection, hotspot_collection, study_area):\n",
        "    \"\"\"\n",
        "    สร้างข้อมูลรวมรายเดือนจาก NDVI และ hotspot ที่กรองแล้ว\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if ndvi_collection is None or hotspot_collection is None:\n",
        "            print(\"ข้อมูล NDVI หรือ hotspot เป็น None\")\n",
        "            return None\n",
        "\n",
        "        ndvi_size = ndvi_collection.size().getInfo()\n",
        "        hotspot_size = hotspot_collection.size().getInfo()\n",
        "        print(f\"NDVI collection size: {ndvi_size}\")\n",
        "        print(f\"Hotspot collection size: {hotspot_size}\")\n",
        "\n",
        "        if ndvi_size == 0 or hotspot_size == 0:\n",
        "            print(\"ไม่มีข้อมูล NDVI หรือ hotspot\")\n",
        "            return None\n",
        "\n",
        "        # รวม NDVI และ hotspot โดยใช้ time_start\n",
        "        def join_ndvi_hotspot(ndvi_image):\n",
        "            time_start = ndvi_image.get('system:time_start')\n",
        "\n",
        "            # หา hotspot image ที่มี time_start เดียวกัน\n",
        "            hotspot_match = hotspot_collection.filter(\n",
        "                ee.Filter.eq('system:time_start', time_start)\n",
        "            ).first()\n",
        "\n",
        "            # รวม bands\n",
        "            combined = ndvi_image.addBands(\n",
        "                ee.Algorithms.If(\n",
        "                    hotspot_match,\n",
        "                    hotspot_match.select('hotspot_count'),\n",
        "                    ee.Image.constant(0).rename('hotspot_count')\n",
        "                )\n",
        "            )\n",
        "\n",
        "            return combined\n",
        "\n",
        "        # รวมข้อมูล NDVI และ hotspot\n",
        "        combined_collection = ndvi_collection.map(join_ndvi_hotspot)\n",
        "\n",
        "        # ลดขนาดข้อมูลเป็นค่าเฉลี่ยของพื้นที่ศึกษา\n",
        "        def reduce_to_features(image):\n",
        "            reduced = image.reduceRegion(\n",
        "                reducer=ee.Reducer.mean(),\n",
        "                geometry=study_area,\n",
        "                scale=1000,\n",
        "                maxPixels=1e9\n",
        "            )\n",
        "\n",
        "            # สร้าง Feature พร้อมวันที่\n",
        "            date_str = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n",
        "            year = ee.Date(image.get('system:time_start')).get('year')\n",
        "            month = ee.Date(image.get('system:time_start')).get('month')\n",
        "\n",
        "            properties = reduced.combine({\n",
        "                'date': date_str,\n",
        "                'year': year,\n",
        "                'month': month\n",
        "            })\n",
        "\n",
        "            return ee.Feature(None, properties)\n",
        "\n",
        "        # แปลงเป็น FeatureCollection\n",
        "        feature_collection = combined_collection.map(reduce_to_features)\n",
        "\n",
        "        return feature_collection\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating combined monthly dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# ฟังก์ชันคำนวณ NDVI จาก MOD09Q1\n",
        "def calculate_ndvi(image):\n",
        "    try:\n",
        "        # MOD09Q1 bands: sur_refl_b01 (red), sur_refl_b02 (NIR)\n",
        "        nir = image.select('sur_refl_b02').multiply(0.0001)  # Apply scale factor\n",
        "        red = image.select('sur_refl_b01').multiply(0.0001)  # Apply scale factor\n",
        "\n",
        "        # คำนวณ NDVI\n",
        "        ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI')\n",
        "\n",
        "        # กำหนดคุณสมบัติให้กับภาพ\n",
        "        return image.addBands(ndvi).copyProperties(image, ['system:time_start'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating NDVI: {e}\")\n",
        "        return None\n",
        "\n",
        "# ฟังก์ชันดึงข้อมูล NDVI รายเดือนจาก MOD09Q1\n",
        "def get_monthly_ndvi(start_date, end_date, study_area):\n",
        "    try:\n",
        "        # เรียกชุดข้อมูล MOD09Q1\n",
        "        modis = ee.ImageCollection('MODIS/061/MOD09Q1') \\\n",
        "            .filterDate(start_date, end_date) \\\n",
        "            .filterBounds(study_area)\n",
        "\n",
        "        # ตรวจสอบว่ามีข้อมูลหรือไม่\n",
        "        size = modis.size().getInfo()\n",
        "        print(f\"จำนวนภาพ MODIS ดิบ: {size}\")\n",
        "        if size == 0:\n",
        "            print(\"ไม่มีข้อมูล MOD09Q1 ในพื้นที่ศึกษา\")\n",
        "            return None\n",
        "\n",
        "        # คำนวณ NDVI\n",
        "        modis_ndvi = modis.map(calculate_ndvi)\n",
        "        print(f\"จำนวนภาพหลังคำนวณ NDVI: {modis_ndvi.size().getInfo()}\")\n",
        "\n",
        "        # เฉลี่ย NDVI เป็นรายเดือน\n",
        "        def create_monthly_composite(year_month):\n",
        "            year = ee.Number(year_month).divide(100).floor().int()\n",
        "            month = ee.Number(year_month).mod(100).int()\n",
        "\n",
        "            start = ee.Date.fromYMD(year, month, 1)\n",
        "            end = start.advance(1, 'month')\n",
        "\n",
        "            monthly_collection = modis_ndvi.filterDate(start, end)\n",
        "            count = monthly_collection.size()\n",
        "\n",
        "            # สร้างภาพเฉลี่ยรายเดือน\n",
        "            monthly_mean = monthly_collection.mean() \\\n",
        "                .set('system:time_start', start.millis()) \\\n",
        "                .set('system:index', start.format('YYYY_MM')) \\\n",
        "                .set('year', year) \\\n",
        "                .set('month', month)\n",
        "\n",
        "            return ee.Algorithms.If(\n",
        "                count.gt(0),\n",
        "                monthly_mean,\n",
        "                ee.Image.constant(0).rename('NDVI') \\\n",
        "                    .set('system:time_start', start.millis()) \\\n",
        "                    .set('system:index', start.format('YYYY_MM')) \\\n",
        "                    .set('year', year) \\\n",
        "                    .set('month', month)\n",
        "            )\n",
        "\n",
        "        # สร้างรายการปี-เดือน\n",
        "        start_year = int(start_date.split('-')[0])\n",
        "        end_year = int(end_date.split('-')[0])\n",
        "        year_months = []\n",
        "\n",
        "        for year in range(start_year, end_year + 1):\n",
        "            start_month = 1 if year > start_year else int(start_date.split('-')[1])\n",
        "            end_month = 12 if year < end_year else int(end_date.split('-')[1])\n",
        "\n",
        "            for month in range(start_month, end_month + 1):\n",
        "                year_months.append(year * 100 + month)\n",
        "\n",
        "        year_months_ee = ee.List(year_months)\n",
        "\n",
        "        # สร้าง ImageCollection\n",
        "        monthly_images = year_months_ee.map(create_monthly_composite)\n",
        "        monthly_composites = ee.ImageCollection.fromImages(monthly_images)\n",
        "\n",
        "        print(f\"จำนวนเดือนทั้งหมด: {monthly_composites.size().getInfo()}\")\n",
        "\n",
        "        return monthly_composites.select('NDVI')\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting monthly NDVI: {e}\")\n",
        "        return None\n",
        "\n",
        "# ฟังก์ชันสร้างข้อมูลรวมรายเดือน - FIXED VERSION\n",
        "def create_monthly_dataset(ndvi_collection, study_area):\n",
        "    try:\n",
        "        # ตรวจสอบว่ามีข้อมูลหรือไม่\n",
        "        if ndvi_collection is None:\n",
        "            print(\"ข้อมูล NDVI เป็น None\")\n",
        "            return None\n",
        "\n",
        "        ndvi_size = ndvi_collection.size().getInfo()\n",
        "        print(f\"NDVI collection size: {ndvi_size}\")\n",
        "\n",
        "        if ndvi_size == 0:\n",
        "            print(\"ไม่มีข้อมูล NDVI\")\n",
        "            return None\n",
        "\n",
        "        # ใช้วันที่ของ NDVI เป็นหลัก\n",
        "        ndvi_times = ndvi_collection.aggregate_array('system:time_start')\n",
        "        print(f\"จำนวนเดือนที่มีข้อมูล NDVI: {ndvi_times.size().getInfo()}\")\n",
        "\n",
        "        # ฟังก์ชันสำหรับรวมข้อมูลแต่ละเดือน - FIXED to avoid client-side operations\n",
        "        def combine_monthly_data(time_start):\n",
        "            # กรองข้อมูล NDVI ตามเวลา\n",
        "            ndvi_image = ndvi_collection.filter(ee.Filter.eq('system:time_start', time_start)).first()\n",
        "\n",
        "            # หา hotspot ในเดือนเดียวกับ NDVI\n",
        "            start_date = ee.Date(time_start)\n",
        "            end_date = start_date.advance(1, 'month')\n",
        "\n",
        "            # ดึงข้อมูล FIRMS ในช่วงเวลาเดียวกับ NDVI\n",
        "            firms_in_period = ee.ImageCollection('FIRMS') \\\n",
        "                .filterDate(start_date, end_date) \\\n",
        "                .filterBounds(study_area)\n",
        "\n",
        "            # 4. Change T21 to count hotspot - Convert to hotspot count\n",
        "            def count_hotspots(image):\n",
        "                return image.select('T21').gt(0).rename('hotspot_presence')\n",
        "\n",
        "            hotspot_presence = firms_in_period.map(count_hotspots)\n",
        "\n",
        "            # Sum hotspots spatially and temporally - FIXED to avoid client-side .getInfo()\n",
        "            monthly_hotspot_sum = hotspot_presence.sum()\n",
        "\n",
        "            # Use ee.Algorithms.If to handle empty collections without client-side operations\n",
        "            has_hotspots = hotspot_presence.size().gt(0)\n",
        "\n",
        "            hotspot_count_reduced = ee.Algorithms.If(\n",
        "                has_hotspots,\n",
        "                monthly_hotspot_sum.reduceRegion(\n",
        "                    reducer=ee.Reducer.sum(),\n",
        "                    geometry=study_area,\n",
        "                    scale=1000,\n",
        "                    maxPixels=1e9\n",
        "                ).get('hotspot_presence'),\n",
        "                0\n",
        "            )\n",
        "\n",
        "            # Create hotspot count image\n",
        "            hotspot_count_image = ee.Image.constant(hotspot_count_reduced).rename('hotspot_count')\n",
        "\n",
        "            # รวมภาพ\n",
        "            combined_image = ndvi_image.addBands(hotspot_count_image)\n",
        "\n",
        "            # ลดขนาดข้อมูลเป็นค่าเฉลี่ยของพื้นที่ศึกษา\n",
        "            reduced = combined_image.reduceRegion(\n",
        "                reducer=ee.Reducer.mean(),\n",
        "                geometry=study_area,\n",
        "                scale=1000,  # 1km resolution\n",
        "                maxPixels=1e9\n",
        "            )\n",
        "\n",
        "            # สร้าง Feature พร้อมวันที่\n",
        "            date_str = start_date.format('YYYY-MM-dd')\n",
        "            year = start_date.get('year')\n",
        "            month = start_date.get('month')\n",
        "\n",
        "            # Create properties dictionary properly\n",
        "            properties = reduced.combine({\n",
        "                'date': date_str,\n",
        "                'year': year,\n",
        "                'month': month\n",
        "            })\n",
        "\n",
        "            return ee.Feature(None, properties)\n",
        "\n",
        "        # แปลงเป็น FeatureCollection โดยใช้เวลาของ NDVI\n",
        "        ndvi_times_size = ndvi_times.size().getInfo()\n",
        "        if ndvi_times_size > 0:\n",
        "            combined_fc = ee.FeatureCollection(ndvi_times.map(combine_monthly_data))\n",
        "            return combined_fc\n",
        "        else:\n",
        "            print(\"ไม่มีข้อมูล NDVI\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating monthly dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# ฟังก์ชันแปลง FeatureCollection เป็น DataFrame\n",
        "def fc_to_df(fc):\n",
        "    try:\n",
        "        # ดึงข้อมูลจาก GEE\n",
        "        features = fc.getInfo()['features']\n",
        "    except Exception as e:\n",
        "        print(\"ไม่สามารถดึงข้อมูลจาก GEE ได้:\", str(e))\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # สร้าง dictionary สำหรับเก็บข้อมูล - Updated to use hotspot_count\n",
        "    data_dict = {'date': [], 'NDVI': [], 'hotspot_count': [], 'year': [], 'month': []}\n",
        "\n",
        "    for feature in features:\n",
        "        props = feature['properties']\n",
        "        if 'NDVI' in props and props['NDVI'] is not None:\n",
        "            data_dict['date'].append(props.get('date', ''))\n",
        "            data_dict['NDVI'].append(props.get('NDVI', 0))\n",
        "            data_dict['hotspot_count'].append(props.get('hotspot_count', 0))\n",
        "            data_dict['year'].append(props.get('year', 0))\n",
        "            data_dict['month'].append(props.get('month', 0))\n",
        "\n",
        "    # สร้าง DataFrame\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # แปลงคอลัมน์ date เป็น datetime\n",
        "    if not df.empty:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df.sort_values('date', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# ฟังก์ชันสร้างแบบจำลอง LSTM สำหรับรายเดือน\n",
        "def create_monthly_lstm_model(sequence_length, n_features):\n",
        "    \"\"\"สร้างแบบจำลอง LSTM สำหรับทำนาย hotspot รายเดือน\"\"\"\n",
        "    try:\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "        from tensorflow.keras.optimizers import Adam\n",
        "        from tensorflow.keras.regularizers import l2\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        # LSTM layers with regularization\n",
        "        model.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, n_features),\n",
        "                      kernel_regularizer=l2(0.001)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(LSTM(32, return_sequences=True,\n",
        "                      kernel_regularizer=l2(0.001)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(LSTM(16, return_sequences=False,\n",
        "                      kernel_regularizer=l2(0.001)))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        # Dense layers\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                      loss='mse',\n",
        "                      metrics=['mae'])\n",
        "\n",
        "        return model\n",
        "    except ImportError:\n",
        "        print(\"ไม่สามารถ import TensorFlow ได้ กรุณาติดตั้ง TensorFlow ก่อน\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างแบบจำลอง: {e}\")\n",
        "        return None\n",
        "\n",
        "# 5. Add learning curve chart\n",
        "def plot_learning_curve(history):\n",
        "    \"\"\"สร้างกราฟ Learning Curve\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss curve\n",
        "        ax1.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "        if 'val_loss' in history.history:\n",
        "            ax1.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "        ax1.set_title(f'Model Loss Learning Curve ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss (MSE)')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # MAE curve\n",
        "        ax2.plot(history.history['mae'], label='Training MAE', color='blue')\n",
        "        if 'val_mae' in history.history:\n",
        "            ax2.plot(history.history['val_mae'], label='Validation MAE', color='red')\n",
        "        ax2.set_title(f'Model MAE Learning Curve ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Mean Absolute Error')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # plt.savefig(f'{siteName}_learning_curve.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"บันทึกกราฟ Learning Curve: learning_curve.png\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ไม่สามารถ import matplotlib ได้\")\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างกราฟ Learning Curve: {e}\")\n",
        "\n",
        "# 6. Add actual vs predict chart\n",
        "def plot_actual_vs_predicted(y_true, y_pred, target_column='hotspot_count'):\n",
        "    \"\"\"สร้างกราฟ Actual vs Predicted\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Scatter plot\n",
        "        ax1.scatter(y_true, y_pred, alpha=0.6, color='blue')\n",
        "\n",
        "        # Perfect prediction line\n",
        "        min_val = min(min(y_true), min(y_pred))\n",
        "        max_val = max(max(y_true), max(y_pred))\n",
        "        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "        # Calculate metrics\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "        ax1.set_xlabel(f'Actual {target_column}')\n",
        "        ax1.set_ylabel(f'Predicted {target_column}')\n",
        "        ax1.set_title(f'Actual vs Predicted {target_column} ({siteName})\\nR² = {r2:.3f}, MAE = {mae:.3f}, RMSE = {rmse:.3f}')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Residual plot\n",
        "        residuals = y_true - y_pred\n",
        "        ax2.scatter(y_pred, residuals, alpha=0.6, color='green')\n",
        "        ax2.axhline(y=0, color='r', linestyle='--')\n",
        "        ax2.set_xlabel(f'Predicted {target_column}')\n",
        "        ax2.set_ylabel('Residuals')\n",
        "        ax2.set_title('Residual Plot')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # plt.savefig(f'{siteName}_actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"บันทึกกราฟ Actual vs Predicted: actual_vs_predicted.png\")\n",
        "\n",
        "        return r2, mae, rmse\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ไม่สามารถ import matplotlib หรือ seaborn ได้\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างกราฟ Actual vs Predicted: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# ฟังก์ชันระบบฤดูกาลแบบเอเชีย\n",
        "def get_asian_season(month):\n",
        "    \"\"\"แบ่งฤดูกาลตามภูมิอากาศเอเชีย (ไทย)\"\"\"\n",
        "    if month in [3, 4, 5]:\n",
        "        return 'Summer'  # ฤดูร้อน\n",
        "    elif month in [6, 7, 8, 9, 10]:\n",
        "        return 'Rainy'   # ฤดูฝน\n",
        "    else:  # month in [11, 12, 1, 2]\n",
        "        return 'Winter'  # ฤดูหนาว\n",
        "\n",
        "def get_asian_season_thai(month):\n",
        "    \"\"\"เวอร์ชันภาษาไทย\"\"\"\n",
        "    if month in [3, 4, 5]:\n",
        "        return 'ฤดูร้อน'\n",
        "    elif month in [6, 7, 8, 9, 10]:\n",
        "        return 'ฤดูฝน'\n",
        "    else:\n",
        "        return 'ฤดูหนาว'\n",
        "\n",
        "def get_asian_season_number(month):\n",
        "    \"\"\"แปลงฤดูกาลเป็นตัวเลข สำหรับการคำนวณ\"\"\"\n",
        "    if month in [3, 4, 5]:\n",
        "        return 1  # Summer\n",
        "    elif month in [6, 7, 8, 9, 10]:\n",
        "        return 2  # Rainy\n",
        "    else:\n",
        "        return 3  # Winter\n",
        "\n",
        "# 2. แทนที่ฟังก์ชัน add_seasonal_features เดิม\n",
        "def add_seasonal_features(df):\n",
        "    \"\"\"เพิ่ม features สำหรับฤดูกาลแบบเอเชีย\"\"\"\n",
        "\n",
        "    # Monthly cyclical encoding (เดิม)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Asian seasonal features - Binary encoding\n",
        "    df['summer_season'] = (df['month'].isin([3, 4, 5])).astype(int)      # มีนาคม-พฤษภาคม\n",
        "    df['rainy_season'] = (df['month'].isin([6, 7, 8, 9, 10])).astype(int)  # มิถุนายน-ตุลาคม\n",
        "    df['winter_season'] = (df['month'].isin([11, 12, 1, 2])).astype(int)   # พฤศจิกายน-กุมภาพันธ์\n",
        "\n",
        "    # Asian seasonal cyclical encoding (3 seasons instead of 4)\n",
        "    df['season_number'] = df['month'].apply(get_asian_season_number)\n",
        "    df['season_sin'] = np.sin(2 * np.pi * df['season_number'] / 3)\n",
        "    df['season_cos'] = np.cos(2 * np.pi * df['season_number'] / 3)\n",
        "\n",
        "    # Add season names for analysis\n",
        "    df['season_name'] = df['month'].apply(get_asian_season)\n",
        "    df['season_name_thai'] = df['month'].apply(get_asian_season_thai)\n",
        "\n",
        "    # Fire season indicators (specific to Thailand fire patterns)\n",
        "    df['peak_fire_season'] = (df['month'].isin([2, 3, 4])).astype(int)  # Peak fire season\n",
        "    df['low_fire_season'] = (df['month'].isin([6, 7, 8, 9])).astype(int)  # Low fire season\n",
        "\n",
        "    return df\n",
        "\n",
        "# 3. แทนที่ฟังก์ชัน prepare_monthly_training_data เดิม\n",
        "def prepare_monthly_training_data(df, sequence_length=6, target_column='hotspot_count'):\n",
        "    \"\"\"เตรียมข้อมูลสำหรับการสร้างแบบจำลองด้วยระบบฤดูกาลแบบเอเชีย\"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"ไม่มีข้อมูลใน DataFrame\")\n",
        "\n",
        "    print(f\"จำนวนข้อมูลดิบ: {len(df)}\")\n",
        "    print(f\"คอลัมน์ที่มี: {df.columns.tolist()}\")\n",
        "    print(f\"ตัวอย่างข้อมูล:\\n{df.head()}\")\n",
        "\n",
        "    # เพิ่ม seasonal features แบบเอเชีย\n",
        "    df = add_seasonal_features(df)\n",
        "\n",
        "    # ล้างข้อมูลที่ขาดหาย\n",
        "    df_cleaned = df.fillna(0)\n",
        "    print(f\"จำนวนข้อมูลหลังล้าง: {len(df_cleaned)}\")\n",
        "    print(f\"ค่าสถิติพื้นฐาน:\\n{df_cleaned.describe()}\")\n",
        "\n",
        "    # ตรวจสอบว่ามีข้อมูลเพียงพอ\n",
        "    min_required = sequence_length + 5\n",
        "    if len(df_cleaned) < min_required:\n",
        "        raise ValueError(f\"ข้อมูลไม่เพียงพอสำหรับการสร้างลำดับ ต้องการอย่างน้อย {min_required} ข้อมูล แต่มีเพียง {len(df_cleaned)}\")\n",
        "\n",
        "    # เตรียมข้อมูล feature และ target - ใช้ features แบบเอเชีย\n",
        "    feature_columns = [\n",
        "        'NDVI',\n",
        "        'month_sin', 'month_cos',           # Monthly cyclical\n",
        "        'season_sin', 'season_cos',         # Asian seasonal cyclical\n",
        "        'summer_season', 'rainy_season', 'winter_season',  # Asian seasonal binary\n",
        "        'peak_fire_season', 'low_fire_season'  # Fire season specific\n",
        "    ]\n",
        "\n",
        "    # ตรวจสอบว่า target column มีอยู่\n",
        "    if target_column not in df_cleaned.columns:\n",
        "        target_column = 'hotspot_count'\n",
        "        print(f\"ใช้ {target_column} เป็น target variable\")\n",
        "\n",
        "    X = df_cleaned[feature_columns].values\n",
        "    y = df_cleaned[[target_column]].values\n",
        "\n",
        "    print(f\"Features ที่ใช้: {feature_columns}\")\n",
        "    print(f\"Target ที่ใช้: {target_column}\")\n",
        "    print(f\"รูปร่างข้อมูล: X = {X.shape}, y = {y.shape}\")\n",
        "\n",
        "    # ปรับขนาดข้อมูล\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler_x = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_scaled = scaler_x.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "    # สร้างลำดับข้อมูลสำหรับ LSTM\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X_scaled) - sequence_length):\n",
        "        X_seq.append(X_scaled[i:i+sequence_length])\n",
        "        y_seq.append(y_scaled[i+sequence_length])\n",
        "\n",
        "    X_seq = np.array(X_seq)\n",
        "    y_seq = np.array(y_seq)\n",
        "\n",
        "    print(f\"จำนวนลำดับที่สร้างได้: {len(X_seq)}\")\n",
        "    print(f\"รูปร่างข้อมูล X: {X_seq.shape}, y: {y_seq.shape}\")\n",
        "\n",
        "    return X_seq, y_seq, scaler_x, scaler_y, df_cleaned\n",
        "\n",
        "# 4. แทนที่ฟังก์ชัน predict_next_12_months เดิม\n",
        "def predict_next_12_months(model, last_sequence, scaler_x, scaler_y, df_cleaned, sequence_length=6):\n",
        "    \"\"\"ทำนายค่า hotspot สำหรับ 12 เดือนถัดไปด้วยระบบฤดูกาลแบบเอเชีย\"\"\"\n",
        "\n",
        "    predictions = []\n",
        "    prediction_dates = []\n",
        "    current_sequence = last_sequence.copy()\n",
        "\n",
        "    # หาวันที่สุดท้ายในข้อมูล\n",
        "    last_date = df_cleaned['date'].iloc[-1]\n",
        "\n",
        "    for month_ahead in range(1, 13):\n",
        "        # ทำนายเดือนถัดไป\n",
        "        pred_scaled = model.predict(current_sequence, verbose=0)\n",
        "        pred_original = scaler_y.inverse_transform(pred_scaled)\n",
        "        predictions.append(pred_original[0][0])\n",
        "\n",
        "        # คำนวณวันที่ของเดือนถัดไป\n",
        "        if last_date.month == 12:\n",
        "            next_year = last_date.year + (last_date.month + month_ahead - 1) // 12\n",
        "            next_month = ((last_date.month + month_ahead - 1) % 12) + 1\n",
        "        else:\n",
        "            next_year = last_date.year + (last_date.month + month_ahead - 1) // 12\n",
        "            next_month = ((last_date.month + month_ahead - 1) % 12) + 1\n",
        "\n",
        "        if next_month > 12:\n",
        "            next_year += 1\n",
        "            next_month = next_month - 12\n",
        "\n",
        "        prediction_date = datetime(next_year, next_month, 1)\n",
        "        prediction_dates.append(prediction_date)\n",
        "\n",
        "        # สร้าง features สำหรับเดือนถัดไป - ใช้ระบบฤดูกาลแบบเอเชีย\n",
        "        month_sin = np.sin(2 * np.pi * next_month / 12)\n",
        "        month_cos = np.cos(2 * np.pi * next_month / 12)\n",
        "\n",
        "        # Asian seasonal features\n",
        "        season_number = get_asian_season_number(next_month)\n",
        "        season_sin = np.sin(2 * np.pi * season_number / 3)\n",
        "        season_cos = np.cos(2 * np.pi * season_number / 3)\n",
        "\n",
        "        summer_season = 1 if next_month in [3, 4, 5] else 0\n",
        "        rainy_season = 1 if next_month in [6, 7, 8, 9, 10] else 0\n",
        "        winter_season = 1 if next_month in [11, 12, 1, 2] else 0\n",
        "\n",
        "        peak_fire_season = 1 if next_month in [2, 3, 4] else 0\n",
        "        low_fire_season = 1 if next_month in [6, 7, 8, 9] else 0\n",
        "\n",
        "        # สมมติค่า NDVI\n",
        "        same_month_data = df_cleaned[df_cleaned['month'] == next_month]\n",
        "        if not same_month_data.empty:\n",
        "            avg_ndvi = same_month_data['NDVI'].mean()\n",
        "        else:\n",
        "            avg_ndvi = df_cleaned['NDVI'].mean()\n",
        "\n",
        "        # สร้าง feature vector สำหรับเดือนถัดไป\n",
        "        next_features = np.array([[\n",
        "            avg_ndvi, month_sin, month_cos, season_sin, season_cos,\n",
        "            summer_season, rainy_season, winter_season,\n",
        "            peak_fire_season, low_fire_season\n",
        "        ]])\n",
        "\n",
        "        # ปรับขนาดข้อมูล\n",
        "        next_features_scaled = scaler_x.transform(next_features)\n",
        "\n",
        "        # อัพเดท sequence สำหรับการทำนายครั้งถัดไป\n",
        "        current_sequence = np.roll(current_sequence, -1, axis=1)\n",
        "        current_sequence[0, -1] = next_features_scaled[0]\n",
        "\n",
        "    return predictions, prediction_dates\n",
        "\n",
        "# ฟังก์ชันสร้างตารางสรุปการทำนาย 12 เดือน - Updated for hotspot_count\n",
        "def create_prediction_summary_table(predictions, prediction_dates, target_column='hotspot_count'):\n",
        "    \"\"\"สร้างตารางสรุปการทำนาย\"\"\"\n",
        "    try:\n",
        "        # สร้าง DataFrame สำหรับการทำนาย\n",
        "        pred_df = pd.DataFrame({\n",
        "            'Date': prediction_dates,\n",
        "            'Month': [calendar.month_name[date.month] for date in prediction_dates],\n",
        "            'Year': [date.year for date in prediction_dates],\n",
        "            f'Predicted_{target_column}': predictions\n",
        "        })\n",
        "\n",
        "        # เพิ่มคอลัมน์ Risk Level - Updated thresholds for hotspot count\n",
        "        def get_risk_level(value):\n",
        "            if value > 50:  # Adjusted for hotspot count\n",
        "                return 'High Risk'\n",
        "            elif value > 25:  # Adjusted for hotspot count\n",
        "                return 'Medium Risk'\n",
        "            else:\n",
        "                return 'Low Risk'\n",
        "\n",
        "        pred_df['Risk_Level'] = pred_df[f'Predicted_{target_column}'].apply(get_risk_level)\n",
        "\n",
        "        # เพิ่มคอลัมน์ฤดู\n",
        "        def get_season(month):\n",
        "            if month in [6, 7, 8, 9, 10]:\n",
        "                return 'Rainy'\n",
        "            elif month in [11, 12, 1, 2]:\n",
        "                return 'Winter'\n",
        "            else:  # month in [3, 4, 5]\n",
        "                return 'Summer'\n",
        "\n",
        "        pred_df['Season'] = pred_df['Month'].apply(get_season)\n",
        "\n",
        "        # จัดรูปแบบตาราง\n",
        "        pred_df[f'Predicted_{target_column}'] = pred_df[f'Predicted_{target_column}'].round(0)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"สรุปการทำนาย HOTSPOT COUNT รายเดือนสำหรับ 12 เดือนข้างหน้า\")\n",
        "        print(\"=\"*80)\n",
        "        print(pred_df.to_string(index=False))\n",
        "\n",
        "        # สถิติสรุป\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"สถิติสรุป\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"ค่าเฉลี่ย: {pred_df[f'Predicted_{target_column}'].mean():.0f} hotspots\")\n",
        "        print(f\"ค่าสูงสุด: {pred_df[f'Predicted_{target_column}'].max():.0f} hotspots ({pred_df.loc[pred_df[f'Predicted_{target_column}'].idxmax(), 'Month']})\")\n",
        "        print(f\"ค่าต่ำสุด: {pred_df[f'Predicted_{target_column}'].min():.0f} hotspots ({pred_df.loc[pred_df[f'Predicted_{target_column}'].idxmin(), 'Month']})\")\n",
        "\n",
        "        # สรุปตามระดับความเสี่ยง\n",
        "        risk_summary = pred_df['Risk_Level'].value_counts()\n",
        "        print(f\"\\nสรุปตามระดับความเสี่ยง:\")\n",
        "        for risk, count in risk_summary.items():\n",
        "            print(f\"  {risk}: {count} เดือน ({count/12*100:.1f}%)\")\n",
        "\n",
        "        # สรุปตามฤดู\n",
        "        season_summary = pred_df.groupby('Season')[f'Predicted_{target_column}'].agg(['mean', 'max', 'min'])\n",
        "        print(f\"\\nสรุปตามฤดู:\")\n",
        "        for season in season_summary.index:\n",
        "            print(f\"  {season}:\")\n",
        "            print(f\"    ค่าเฉลี่ย: {season_summary.loc[season, 'mean']:.0f} hotspots\")\n",
        "            print(f\"    ค่าสูงสุด: {season_summary.loc[season, 'max']:.0f} hotspots\")\n",
        "            print(f\"    ค่าต่ำสุด: {season_summary.loc[season, 'min']:.0f} hotspots\")\n",
        "\n",
        "        # บันทึกตารางเป็นไฟล์ CSV\n",
        "        # pred_df.to_csv(f'{siteName}_12_month_hotspot_predictions.csv', index=False)\n",
        "        # print(f\"\\nบันทึกตารางการทำนาย: {siteName}_12_month_hotspot_predictions.csv\")\n",
        "\n",
        "        return pred_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างตารางสรุป: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_ndvi_hotspot_time_series(df, save_plot=True):\n",
        "    \"\"\"\n",
        "    Create comprehensive time series plots showing NDVI vs Hotspot count relationship\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        from datetime import datetime\n",
        "        import numpy as np\n",
        "\n",
        "        # Set style for better looking plots\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        # Create figure with multiple subplots\n",
        "        fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "        # 1. Main time series plot with dual y-axes\n",
        "        ax1 = plt.subplot(3, 2, (1, 2))  # Top row, spans 2 columns\n",
        "\n",
        "        # Plot NDVI on primary y-axis\n",
        "        line1 = ax1.plot(df['date'], df['NDVI'],\n",
        "                        color='green', linewidth=2, marker='o', markersize=4,\n",
        "                        label='NDVI', alpha=0.8)\n",
        "        ax1.set_xlabel('Date', fontsize=12)\n",
        "        ax1.set_ylabel('NDVI', fontsize=12, color='green')\n",
        "        ax1.tick_params(axis='y', labelcolor='green')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Create secondary y-axis for hotspot count\n",
        "        ax2 = ax1.twinx()\n",
        "        line2 = ax2.plot(df['date'], df['hotspot_count'],\n",
        "                        color='red', linewidth=2, marker='s', markersize=4,\n",
        "                        label='Hotspot Count', alpha=0.8)\n",
        "        ax2.set_ylabel('Hotspot Count', fontsize=12, color='red')\n",
        "        ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        # Add correlation coefficient to title\n",
        "        correlation = df['NDVI'].corr(df['hotspot_count'])\n",
        "        ax1.set_title(f'NDVI vs Hotspot Count Time Series ({siteName})\\nCorrelation: {correlation:.3f}',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "        # Combine legends\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax1.legend(lines, labels, loc='upper left', fontsize=10)\n",
        "\n",
        "        # Rotate x-axis labels for better readability\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 2. Scatter plot showing correlation\n",
        "        ax3 = plt.subplot(3, 2, 3)\n",
        "        scatter = ax3.scatter(df['NDVI'], df['hotspot_count'],\n",
        "                             c=df.index, cmap='viridis', alpha=0.6, s=50)\n",
        "        ax3.set_xlabel('NDVI', fontsize=12)\n",
        "        ax3.set_ylabel('Hotspot Count', fontsize=12)\n",
        "        ax3.set_title(f'NDVI vs Hotspot Count Scatter Plot ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add trend line\n",
        "        z = np.polyfit(df['NDVI'], df['hotspot_count'], 1)\n",
        "        p = np.poly1d(z)\n",
        "        ax3.plot(df['NDVI'], p(df['NDVI']), \"r--\", alpha=0.8, linewidth=2)\n",
        "\n",
        "        # Add colorbar for time progression\n",
        "        cbar = plt.colorbar(scatter, ax=ax3)\n",
        "        cbar.set_label('Time Progression', fontsize=10)\n",
        "\n",
        "        # 3. Monthly averages comparison\n",
        "        ax4 = plt.subplot(3, 2, 4)\n",
        "        monthly_avg = df.groupby('month').agg({\n",
        "            'NDVI': 'mean',\n",
        "            'hotspot_count': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        # Create month names\n",
        "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "        monthly_avg['month_name'] = [month_names[i-1] for i in monthly_avg['month']]\n",
        "\n",
        "        ax4_twin = ax4.twinx()\n",
        "\n",
        "        bars1 = ax4.bar([x - 0.2 for x in range(len(monthly_avg))], monthly_avg['NDVI'],\n",
        "                       width=0.4, color='green', alpha=0.7, label='NDVI')\n",
        "        bars2 = ax4_twin.bar([x + 0.2 for x in range(len(monthly_avg))], monthly_avg['hotspot_count'],\n",
        "                            width=0.4, color='red', alpha=0.7, label='Hotspot Count')\n",
        "\n",
        "        ax4.set_xlabel('Month', fontsize=12)\n",
        "        ax4.set_ylabel('Average NDVI', fontsize=12, color='green')\n",
        "        ax4_twin.set_ylabel('Average Hotspot Count', fontsize=12, color='red')\n",
        "        ax4.set_title(f'Monthly Averages Comparison ({siteName})', fontsize=14, fontweight='bold')\n",
        "\n",
        "        ax4.set_xticks(range(len(monthly_avg)))\n",
        "        ax4.set_xticklabels(monthly_avg['month_name'], rotation=45)\n",
        "        ax4.tick_params(axis='y', labelcolor='green')\n",
        "        ax4_twin.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        # Add legends\n",
        "        ax4.legend(loc='upper left')\n",
        "        ax4_twin.legend(loc='upper right')\n",
        "\n",
        "        # 4. Seasonal analysis\n",
        "        ax5 = plt.subplot(3, 2, 5)\n",
        "\n",
        "        # Define seasons\n",
        "        def get_season_name(month):\n",
        "            if month in [12, 1, 2]:\n",
        "                return 'Winter'\n",
        "            elif month in [3, 4, 5]:\n",
        "                return 'Spring'\n",
        "            elif month in [6, 7, 8]:\n",
        "                return 'Summer'\n",
        "            else:\n",
        "                return 'Autumn'\n",
        "\n",
        "        df['season'] = df['month'].apply(get_season_name)\n",
        "        seasonal_stats = df.groupby('season').agg({\n",
        "            'NDVI': ['mean', 'std'],\n",
        "            'hotspot_count': ['mean', 'std']\n",
        "        }).round(3)\n",
        "\n",
        "        seasons = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
        "        ndvi_means = [seasonal_stats.loc[s, ('NDVI', 'mean')] for s in seasons]\n",
        "        ndvi_stds = [seasonal_stats.loc[s, ('NDVI', 'std')] for s in seasons]\n",
        "        hotspot_means = [seasonal_stats.loc[s, ('hotspot_count', 'mean')] for s in seasons]\n",
        "        hotspot_stds = [seasonal_stats.loc[s, ('hotspot_count', 'std')] for s in seasons]\n",
        "\n",
        "        x_pos = np.arange(len(seasons))\n",
        "\n",
        "        ax5_twin = ax5.twinx()\n",
        "\n",
        "        bars1 = ax5.bar(x_pos - 0.2, ndvi_means, 0.4, yerr=ndvi_stds,\n",
        "                       color='green', alpha=0.7, capsize=5, label='NDVI')\n",
        "        bars2 = ax5_twin.bar(x_pos + 0.2, hotspot_means, 0.4, yerr=hotspot_stds,\n",
        "                            color='red', alpha=0.7, capsize=5, label='Hotspot Count')\n",
        "\n",
        "        ax5.set_xlabel('Season', fontsize=12)\n",
        "        ax5.set_ylabel('Average NDVI', fontsize=12, color='green')\n",
        "        ax5_twin.set_ylabel('Average Hotspot Count', fontsize=12, color='red')\n",
        "        ax5.set_title(f'Seasonal Analysis with Standard Deviation ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax5.set_xticks(x_pos)\n",
        "        ax5.set_xticklabels(seasons)\n",
        "        ax5.tick_params(axis='y', labelcolor='green')\n",
        "        ax5_twin.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        ax5.legend(loc='upper left')\n",
        "        ax5_twin.legend(loc='upper right')\n",
        "\n",
        "        # 5. Moving averages\n",
        "        ax6 = plt.subplot(3, 2, 6)\n",
        "\n",
        "        # Calculate moving averages\n",
        "        window = 6  # 6-month moving average\n",
        "        df['NDVI_ma'] = df['NDVI'].rolling(window=window, center=True).mean()\n",
        "        df['hotspot_ma'] = df['hotspot_count'].rolling(window=window, center=True).mean()\n",
        "\n",
        "        line1 = ax6.plot(df['date'], df['NDVI_ma'],\n",
        "                        color='darkgreen', linewidth=3, label=f'NDVI {window}-month MA')\n",
        "\n",
        "        ax6_twin = ax6.twinx()\n",
        "        line2 = ax6_twin.plot(df['date'], df['hotspot_ma'],\n",
        "                             color='darkred', linewidth=3, label=f'Hotspot {window}-month MA')\n",
        "\n",
        "        ax6.set_xlabel('Date', fontsize=12)\n",
        "        ax6.set_ylabel('NDVI Moving Average', fontsize=12, color='darkgreen')\n",
        "        ax6_twin.set_ylabel('Hotspot Moving Average', fontsize=12, color='darkred')\n",
        "        ax6.set_title(f'{window}-Month Moving Averages ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax6.tick_params(axis='x', rotation=45)\n",
        "        ax6.tick_params(axis='y', labelcolor='darkgreen')\n",
        "        ax6_twin.tick_params(axis='y', labelcolor='darkred')\n",
        "        ax6.grid(True, alpha=0.3)\n",
        "\n",
        "        ax6.legend(loc='upper left')\n",
        "        ax6_twin.legend(loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_plot:\n",
        "            # plt.savefig(f'{siteName}_ndvi_hotspot_time_series_analysis.png', dpi=300, bbox_inches='tight')\n",
        "            print(f\"บันทึกกราฟ NDVI vs Hotspot Time Series: {siteName}_ndvi_hotspot_time_series_analysis.png\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Print correlation analysis\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"การวิเคราะห์ความสัมพันธ์ NDVI vs Hotspot Count\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Pearson Correlation: {correlation:.4f}\")\n",
        "\n",
        "        # Lag correlation analysis\n",
        "        print(\"\\nLag Correlation Analysis:\")\n",
        "        for lag in range(1, 7):\n",
        "            lag_corr = df['NDVI'].corr(df['hotspot_count'].shift(lag))\n",
        "            print(f\"NDVI vs Hotspot (lag {lag} months): {lag_corr:.4f}\")\n",
        "\n",
        "        # Seasonal correlation\n",
        "        print(f\"\\nSeasonal Correlations:\")\n",
        "        for season in seasons:\n",
        "            season_data = df[df['season'] == season]\n",
        "            if len(season_data) > 1:\n",
        "                season_corr = season_data['NDVI'].corr(season_data['hotspot_count'])\n",
        "                print(f\"{season}: {season_corr:.4f}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ไม่สามารถ import matplotlib หรือ seaborn ได้\")\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างกราฟ time series: {e}\")\n",
        "\n",
        "\n",
        "def plot_yearly_comparison(df, save_plot=True):\n",
        "    \"\"\"\n",
        "    Create yearly comparison plots for NDVI and Hotspot trends\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        # Extract year from date\n",
        "        df['year'] = df['date'].dt.year\n",
        "        years = sorted(df['year'].unique())\n",
        "\n",
        "        fig, ((ax3, ax2), (ax1, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "        # 1. Yearly averages line plot\n",
        "        yearly_avg = df.groupby('year').agg({\n",
        "            'NDVI': 'mean',\n",
        "            'hotspot_count': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        ax1_twin = ax1.twinx()\n",
        "\n",
        "        line1 = ax1.plot(yearly_avg['year'], yearly_avg['NDVI'],\n",
        "                        color='green', marker='o', linewidth=3, markersize=8, label='NDVI')\n",
        "        line2 = ax1_twin.plot(yearly_avg['year'], yearly_avg['hotspot_count'],\n",
        "                             color='red', marker='s', linewidth=3, markersize=8, label='Hotspot Count')\n",
        "\n",
        "        ax1.set_xlabel('Year', fontsize=12)\n",
        "        ax1.set_ylabel('Average NDVI', fontsize=12, color='green')\n",
        "        ax1_twin.set_ylabel('Average Hotspot Count', fontsize=12, color='red')\n",
        "        ax1.set_title(f'Yearly Averages Trend ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax1.tick_params(axis='y', labelcolor='green')\n",
        "        ax1_twin.tick_params(axis='y', labelcolor='red')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax1.legend(lines, labels, loc='upper left')\n",
        "\n",
        "        # 2. Heatmap of monthly values by year for NDVI\n",
        "        pivot_ndvi = df.pivot_table(values='NDVI', index='year', columns='month', aggfunc='mean')\n",
        "        sns.heatmap(pivot_ndvi, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax2, cbar_kws={'label': 'NDVI'})\n",
        "        ax2.set_title(f'NDVI Monthly Heatmap by Year ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('Month', fontsize=12)\n",
        "        ax2.set_ylabel('Year', fontsize=12)\n",
        "\n",
        "        # 3. Heatmap of monthly values by year for Hotspot Count\n",
        "        pivot_hotspot = df.pivot_table(values='hotspot_count', index='year', columns='month', aggfunc='mean')\n",
        "        sns.heatmap(pivot_hotspot, annot=True, fmt='.1f', cmap='Reds', ax=ax3, cbar_kws={'label': 'Hotspot Count'})\n",
        "        ax3.set_title(f'Hotspot Count Monthly Heatmap by Year ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax3.set_xlabel('Month', fontsize=12)\n",
        "        ax3.set_ylabel('Year', fontsize=12)\n",
        "\n",
        "        # 4. Box plots for distribution comparison\n",
        "        years_to_plot = years[-5:] if len(years) > 5 else years  # Last 5 years or all if less\n",
        "\n",
        "        df_recent = df[df['year'].isin(years_to_plot)]\n",
        "\n",
        "        ax4_twin = ax4.twinx()\n",
        "\n",
        "        # NDVI box plot\n",
        "        ndvi_data = [df_recent[df_recent['year'] == year]['NDVI'].values for year in years_to_plot]\n",
        "        bp1 = ax4.boxplot(ndvi_data, positions=[x - 0.2 for x in range(len(years_to_plot))],\n",
        "                         widths=0.3, patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
        "\n",
        "        # Hotspot box plot\n",
        "        hotspot_data = [df_recent[df_recent['year'] == year]['hotspot_count'].values for year in years_to_plot]\n",
        "        bp2 = ax4_twin.boxplot(hotspot_data, positions=[x + 0.2 for x in range(len(years_to_plot))],\n",
        "                              widths=0.3, patch_artist=True, boxprops=dict(facecolor='lightcoral'))\n",
        "\n",
        "        ax4.set_xlabel('Year', fontsize=12)\n",
        "        ax4.set_ylabel('NDVI Distribution', fontsize=12, color='green')\n",
        "        ax4_twin.set_ylabel('Hotspot Count Distribution', fontsize=12, color='red')\n",
        "        ax4.set_title(f'Distribution Comparison (Last {len(years_to_plot)} Years) ({siteName})', fontsize=14, fontweight='bold')\n",
        "        ax4.set_xticks(range(len(years_to_plot)))\n",
        "        ax4.set_xticklabels(years_to_plot)\n",
        "        ax4.tick_params(axis='y', labelcolor='green')\n",
        "        ax4_twin.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        # Add legend for box plots\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [Patch(facecolor='lightgreen', label='NDVI'),\n",
        "                          Patch(facecolor='lightcoral', label='Hotspot Count')]\n",
        "        ax4.legend(handles=legend_elements, loc='upper left')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_plot:\n",
        "            # plt.savefig(f'{siteName}_yearly_comparison_analysis.png', dpi=300, bbox_inches='tight')\n",
        "            print(f\"บันทึกกราฟ Yearly Comparison: {siteName}_yearly_comparison_analysis.png\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ไม่สามารถ import matplotlib หรือ seaborn ได้\")\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างกราฟ yearly comparison: {e}\")\n",
        "\n",
        "\n",
        "def create_comprehensive_analysis_report(df, save_report=True):\n",
        "    \"\"\"\n",
        "    Create a comprehensive statistical report of NDVI vs Hotspot relationship\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        from scipy import stats\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE ANALYSIS REPORT: NDVI vs HOTSPOT COUNT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Basic statistics\n",
        "        print(\"\\n1. BASIC STATISTICS\")\n",
        "        print(\"-\" * 40)\n",
        "        stats_df = df[['NDVI', 'hotspot_count']].describe()\n",
        "        print(stats_df)\n",
        "\n",
        "        # Correlation analysis\n",
        "        print(\"\\n2. CORRELATION ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        correlation_matrix = df[['NDVI', 'hotspot_count', 'month', 'year']].corr()\n",
        "        print(\"Correlation Matrix:\")\n",
        "        print(correlation_matrix)\n",
        "\n",
        "        # Statistical significance test\n",
        "        corr_coef, p_value = stats.pearsonr(df['NDVI'], df['hotspot_count'])\n",
        "        print(f\"\\nPearson Correlation Coefficient: {corr_coef:.4f}\")\n",
        "        print(f\"P-value: {p_value:.4f}\")\n",
        "        print(f\"Statistical Significance: {'Yes' if p_value < 0.05 else 'No'} (α = 0.05)\")\n",
        "\n",
        "        # Seasonal analysis - แก้ไขให้ใช้ SEA climate\n",
        "        print(\"\\n3. SEASONAL ANALYSIS (SEA Climate)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Define SEA seasons - ปรับให้เหมาะกับเอเชียตะวันออกเฉียงใต้\n",
        "        def get_sea_season(month):\n",
        "            if month in [6, 7, 8, 9, 10]:\n",
        "                return 'Rainy'\n",
        "            elif month in [11, 12, 1, 2]:\n",
        "                return 'Winter'\n",
        "            else:  # month in [3, 4, 5]\n",
        "                return 'Summer'\n",
        "\n",
        "        df['season'] = df['month'].apply(get_sea_season)\n",
        "\n",
        "        seasonal_analysis = df.groupby('season').agg({\n",
        "            'NDVI': ['mean', 'std', 'min', 'max'],\n",
        "            'hotspot_count': ['mean', 'std', 'min', 'max']\n",
        "        }).round(4)\n",
        "\n",
        "        print(\"Seasonal Statistics (SEA Climate):\")\n",
        "        print(seasonal_analysis)\n",
        "\n",
        "        # Monthly trends\n",
        "        print(\"\\n4. MONTHLY TRENDS\")\n",
        "        print(\"-\" * 40)\n",
        "        monthly_analysis = df.groupby('month').agg({\n",
        "            'NDVI': ['mean', 'std'],\n",
        "            'hotspot_count': ['mean', 'std']\n",
        "        }).round(4)\n",
        "\n",
        "        print(\"Monthly Statistics:\")\n",
        "        print(monthly_analysis)\n",
        "\n",
        "        # Identify peak months\n",
        "        peak_ndvi_month = df.groupby('month')['NDVI'].mean().idxmax()\n",
        "        peak_hotspot_month = df.groupby('month')['hotspot_count'].mean().idxmax()\n",
        "        low_ndvi_month = df.groupby('month')['NDVI'].mean().idxmin()\n",
        "        low_hotspot_month = df.groupby('month')['hotspot_count'].mean().idxmin()\n",
        "\n",
        "        month_names = {1: 'January', 2: 'February', 3: 'March', 4: 'April',\n",
        "                      5: 'May', 6: 'June', 7: 'July', 8: 'August',\n",
        "                      9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n",
        "\n",
        "        print(f\"\\nPeak NDVI Month: {month_names[peak_ndvi_month]} ({peak_ndvi_month})\")\n",
        "        print(f\"Lowest NDVI Month: {month_names[low_ndvi_month]} ({low_ndvi_month})\")\n",
        "        print(f\"Peak Hotspot Month: {month_names[peak_hotspot_month]} ({peak_hotspot_month})\")\n",
        "        print(f\"Lowest Hotspot Month: {month_names[low_hotspot_month]} ({low_hotspot_month})\")\n",
        "\n",
        "        # Trend analysis\n",
        "        print(\"\\n5. TREND ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Linear regression for trends\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "\n",
        "        # NDVI trend over time\n",
        "        X = df.index.values.reshape(-1, 1)\n",
        "        y_ndvi = df['NDVI'].values\n",
        "        y_hotspot = df['hotspot_count'].values\n",
        "\n",
        "        reg_ndvi = LinearRegression().fit(X, y_ndvi)\n",
        "        reg_hotspot = LinearRegression().fit(X, y_hotspot)\n",
        "\n",
        "        ndvi_slope = reg_ndvi.coef_[0]\n",
        "        hotspot_slope = reg_hotspot.coef_[0]\n",
        "\n",
        "        print(f\"NDVI Trend: {ndvi_slope:.6f} units per month\")\n",
        "        print(f\"Hotspot Trend: {hotspot_slope:.4f} count per month\")\n",
        "\n",
        "        trend_direction_ndvi = \"Increasing\" if ndvi_slope > 0 else \"Decreasing\" if ndvi_slope < 0 else \"Stable\"\n",
        "        trend_direction_hotspot = \"Increasing\" if hotspot_slope > 0 else \"Decreasing\" if hotspot_slope < 0 else \"Stable\"\n",
        "\n",
        "        print(f\"NDVI Overall Trend: {trend_direction_ndvi}\")\n",
        "        print(f\"Hotspot Overall Trend: {trend_direction_hotspot}\")\n",
        "\n",
        "        # Anomaly detection\n",
        "        print(\"\\n6. ANOMALY DETECTION\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Define anomalies as values beyond 2 standard deviations\n",
        "        ndvi_mean, ndvi_std = df['NDVI'].mean(), df['NDVI'].std()\n",
        "        hotspot_mean, hotspot_std = df['hotspot_count'].mean(), df['hotspot_count'].std()\n",
        "\n",
        "        ndvi_anomalies = df[(df['NDVI'] < ndvi_mean - 2*ndvi_std) | (df['NDVI'] > ndvi_mean + 2*ndvi_std)]\n",
        "        hotspot_anomalies = df[(df['hotspot_count'] < hotspot_mean - 2*hotspot_std) | (df['hotspot_count'] > hotspot_mean + 2*hotspot_std)]\n",
        "\n",
        "        print(f\"NDVI Anomalies: {len(ndvi_anomalies)} records\")\n",
        "        print(f\"Hotspot Anomalies: {len(hotspot_anomalies)} records\")\n",
        "\n",
        "        if len(ndvi_anomalies) > 0:\n",
        "            print(\"\\nNDVI Anomaly Dates:\")\n",
        "            for idx, row in ndvi_anomalies.iterrows():\n",
        "                print(f\"  {row['date'].strftime('%Y-%m-%d')}: NDVI = {row['NDVI']:.4f}\")\n",
        "\n",
        "        if len(hotspot_anomalies) > 0:\n",
        "            print(\"\\nHotspot Anomaly Dates:\")\n",
        "            for idx, row in hotspot_anomalies.iterrows():\n",
        "                print(f\"  {row['date'].strftime('%Y-%m-%d')}: Hotspot Count = {row['hotspot_count']:.0f}\")\n",
        "\n",
        "\n",
        "        return {\n",
        "            'correlation': corr_coef,\n",
        "            'p_value': p_value,\n",
        "            'seasonal_stats': seasonal_analysis,\n",
        "            'monthly_stats': monthly_analysis,\n",
        "            'trends': {'ndvi_slope': ndvi_slope, 'hotspot_slope': hotspot_slope},\n",
        "            'anomalies': {'ndvi': ndvi_anomalies, 'hotspot': hotspot_anomalies}\n",
        "        }\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"ไม่สามารถ import required libraries: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างรายงาน: {e}\")\n",
        "\n",
        "class BaselineModels:\n",
        "    \"\"\"\n",
        "    คลาสสำหรับ baseline models ต่างๆ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, target_column='hotspot_count', date_column='date'):\n",
        "        self.df = df.copy()\n",
        "        self.target_column = target_column\n",
        "        self.date_column = date_column\n",
        "        self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"เตรียมข้อมูลสำหรับ time series modeling\"\"\"\n",
        "        # เรียงลำดับตามวันที่\n",
        "        self.df = self.df.sort_values(self.date_column).reset_index(drop=True)\n",
        "\n",
        "        # สร้าง time series\n",
        "        self.ts = pd.Series(\n",
        "            self.df[self.target_column].values,\n",
        "            index=pd.to_datetime(self.df[self.date_column])\n",
        "        )\n",
        "\n",
        "        # เพิ่มข้อมูลเสริม\n",
        "        self.df['year'] = pd.to_datetime(self.df[self.date_column]).dt.year\n",
        "        self.df['month'] = pd.to_datetime(self.df[self.date_column]).dt.month\n",
        "        self.df['quarter'] = pd.to_datetime(self.df[self.date_column]).dt.quarter\n",
        "\n",
        "    def seasonal_naive(self, train_size=0.8, seasonal_period=12):\n",
        "        \"\"\"\n",
        "        Seasonal Naïve Model: ใช้ค่าจากเดือนเดียวกันในปีก่อน\n",
        "        \"\"\"\n",
        "        split_idx = int(len(self.df) * train_size)\n",
        "        train_data = self.df.iloc[:split_idx]\n",
        "        test_data = self.df.iloc[split_idx:]\n",
        "\n",
        "        predictions = []\n",
        "        actuals = test_data[self.target_column].values\n",
        "\n",
        "        for i, row in test_data.iterrows():\n",
        "            # หาค่าจากเดือนเดียวกันในปีก่อน\n",
        "            target_month = row['month']\n",
        "            target_year = row['year']\n",
        "\n",
        "            # หาข้อมูลในเดือนเดียวกันจากปีก่อนๆ\n",
        "            historical_same_month = train_data[\n",
        "                (train_data['month'] == target_month) &\n",
        "                (train_data['year'] < target_year)\n",
        "            ]\n",
        "\n",
        "            if len(historical_same_month) > 0:\n",
        "                # ใช้ค่าล่าสุดจากเดือนเดียวกัน\n",
        "                pred_value = historical_same_month[self.target_column].iloc[-1]\n",
        "            else:\n",
        "                # ถ้าไม่มีข้อมูลในเดือนเดียวกัน ใช้ค่าเฉลี่ยของเดือนนั้น\n",
        "                pred_value = train_data[self.target_column].mean()\n",
        "\n",
        "            predictions.append(pred_value)\n",
        "\n",
        "        return np.array(predictions), actuals, test_data[self.date_column].values\n",
        "\n",
        "    def naive_drift(self, train_size=0.8):\n",
        "        \"\"\"\n",
        "        Naïve with Drift: ใช้ trend จากข้อมูลในอดีต\n",
        "        \"\"\"\n",
        "        split_idx = int(len(self.df) * train_size)\n",
        "        train_data = self.df.iloc[:split_idx]\n",
        "        test_data = self.df.iloc[split_idx:]\n",
        "\n",
        "        # คำนวณ drift (slope) จากข้อมูล training\n",
        "        y_train = train_data[self.target_column].values\n",
        "        x_train = np.arange(len(y_train))\n",
        "        slope = np.polyfit(x_train, y_train, 1)[0]\n",
        "\n",
        "        # ทำนายด้วย last value + drift\n",
        "        last_value = y_train[-1]\n",
        "        predictions = []\n",
        "\n",
        "        for h in range(1, len(test_data) + 1):\n",
        "            pred = last_value + h * slope\n",
        "            predictions.append(max(0, pred))  # ป้องกันค่าติดลบ\n",
        "\n",
        "        actuals = test_data[self.target_column].values\n",
        "        return np.array(predictions), actuals, test_data[self.date_column].values\n",
        "\n",
        "    def moving_average(self, train_size=0.8, window=6):\n",
        "        \"\"\"\n",
        "        Moving Average Model\n",
        "        \"\"\"\n",
        "        split_idx = int(len(self.df) * train_size)\n",
        "        train_data = self.df.iloc[:split_idx]\n",
        "        test_data = self.df.iloc[split_idx:]\n",
        "\n",
        "        predictions = []\n",
        "        actuals = test_data[self.target_column].values\n",
        "\n",
        "        # รวมข้อมูล train และ test สำหรับการคำนวณ moving average\n",
        "        all_data = self.df[self.target_column].values\n",
        "\n",
        "        for i in range(split_idx, len(self.df)):\n",
        "            if i >= window:\n",
        "                pred = np.mean(all_data[i-window:i])\n",
        "            else:\n",
        "                pred = np.mean(all_data[:i])\n",
        "            predictions.append(pred)\n",
        "\n",
        "        return np.array(predictions), actuals, test_data[self.date_column].values\n",
        "\n",
        "    def exponential_smoothing(self, train_size=0.8):\n",
        "        \"\"\"\n",
        "        Exponential Smoothing (ETS) Model\n",
        "        \"\"\"\n",
        "        if not STATSMODELS_AVAILABLE:\n",
        "            print(\"statsmodels ไม่พร้อมใช้งาน ข้าม ETS model\")\n",
        "            return None, None, None\n",
        "\n",
        "        split_idx = int(len(self.ts) * train_size)\n",
        "        train_ts = self.ts.iloc[:split_idx]\n",
        "        test_ts = self.ts.iloc[split_idx:]\n",
        "\n",
        "        try:\n",
        "            # ลอง Holt-Winters สำหรับข้อมูลที่มี seasonality\n",
        "            model = ExponentialSmoothing(\n",
        "                train_ts,\n",
        "                trend='add',\n",
        "                seasonal='add',\n",
        "                seasonal_periods=12\n",
        "            ).fit()\n",
        "\n",
        "            predictions = model.forecast(len(test_ts))\n",
        "\n",
        "            # คำนวณ prediction intervals\n",
        "            pred_int = model.get_prediction(\n",
        "                start=len(train_ts),\n",
        "                end=len(train_ts) + len(test_ts) - 1\n",
        "            ).conf_int()\n",
        "\n",
        "            return predictions.values, test_ts.values, test_ts.index.values, pred_int\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ETS model ล้มเหลว: {e}\")\n",
        "            # ลองใช้ Simple Exponential Smoothing แทน\n",
        "            try:\n",
        "                model = ExponentialSmoothing(train_ts, trend=None, seasonal=None).fit()\n",
        "                predictions = model.forecast(len(test_ts))\n",
        "                return predictions.values, test_ts.values, test_ts.index.values, None\n",
        "            except:\n",
        "                return None, None, None, None\n",
        "\n",
        "    def arima_model(self, train_size=0.8, order=(1,1,1)):\n",
        "        \"\"\"\n",
        "        ARIMA Model\n",
        "        \"\"\"\n",
        "        if not STATSMODELS_AVAILABLE:\n",
        "            print(\"statsmodels ไม่พร้อมใช้งาน ข้าม ARIMA model\")\n",
        "            return None, None, None\n",
        "\n",
        "        split_idx = int(len(self.ts) * train_size)\n",
        "        train_ts = self.ts.iloc[:split_idx]\n",
        "        test_ts = self.ts.iloc[split_idx:]\n",
        "\n",
        "        try:\n",
        "            model = ARIMA(train_ts, order=order).fit()\n",
        "\n",
        "            # ทำนาย\n",
        "            forecast_result = model.get_forecast(steps=len(test_ts))\n",
        "            predictions = forecast_result.predicted_mean.values\n",
        "\n",
        "            # Prediction intervals\n",
        "            pred_int = forecast_result.conf_int()\n",
        "\n",
        "            return predictions, test_ts.values, test_ts.index.values, pred_int\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ARIMA model ล้มเหลว: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def sarima_model(self, train_size=0.8, order=(1,1,1), seasonal_order=(1,1,1,12)):\n",
        "        \"\"\"\n",
        "        SARIMA Model\n",
        "        \"\"\"\n",
        "        if not STATSMODELS_AVAILABLE:\n",
        "            print(\"statsmodels ไม่พร้อมใช้งาน ข้าม SARIMA model\")\n",
        "            return None, None, None\n",
        "\n",
        "        split_idx = int(len(self.ts) * train_size)\n",
        "        train_ts = self.ts.iloc[:split_idx]\n",
        "        test_ts = self.ts.iloc[split_idx:]\n",
        "\n",
        "        try:\n",
        "            model = SARIMAX(\n",
        "                train_ts,\n",
        "                order=order,\n",
        "                seasonal_order=seasonal_order\n",
        "            ).fit(disp=False)\n",
        "\n",
        "            # ทำนาย\n",
        "            forecast_result = model.get_forecast(steps=len(test_ts))\n",
        "            predictions = forecast_result.predicted_mean.values\n",
        "\n",
        "            # Prediction intervals\n",
        "            pred_int = forecast_result.conf_int()\n",
        "\n",
        "            return predictions, test_ts.values, test_ts.index.values, pred_int\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"SARIMA model ล้มเหลว: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def auto_arima(self, train_size=0.8):\n",
        "        \"\"\"\n",
        "        Auto-ARIMA Model (หาพารามิเตอร์ที่ดีที่สุดอัตโนมัติ)\n",
        "        \"\"\"\n",
        "        if not PMDARIMA_AVAILABLE:\n",
        "            print(\"pmdarima ไม่พร้อมใช้งาน ข้าม Auto-ARIMA model\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        split_idx = int(len(self.ts) * train_size)\n",
        "        train_ts = self.ts.iloc[:split_idx]\n",
        "        test_ts = self.ts.iloc[split_idx:]\n",
        "\n",
        "        try:\n",
        "            # หาโมเดลที่ดีที่สุดอัตโนมัติ\n",
        "            model = pm.auto_arima(\n",
        "                train_ts,\n",
        "                seasonal=True,\n",
        "                m=12,  # seasonal period\n",
        "                max_p=3, max_q=3, max_P=2, max_Q=2,\n",
        "                suppress_warnings=True,\n",
        "                stepwise=True,\n",
        "                error_action='ignore'\n",
        "            )\n",
        "\n",
        "            # ทำนาย\n",
        "            predictions, conf_int = model.predict(\n",
        "                n_periods=len(test_ts),\n",
        "                return_conf_int=True\n",
        "            )\n",
        "\n",
        "            return predictions, test_ts.values, test_ts.index.values, conf_int\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Auto-ARIMA model ล้มเหลว: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "# แก้ปัญหา MAPE calculation\n",
        "def safe_mape(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    คำนวณ MAPE โดยหลีกเลี่ยงปัญหาการหารด้วยศูนย์\n",
        "    \"\"\"\n",
        "    mask = y_true > 0.1  # หลีกเลี่ยงค่าใกล้ศูนย์\n",
        "    if np.sum(mask) == 0:  # ถ้าไม่มีค่าที่ใช้ได้\n",
        "        return 0.0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "def calculate_metrics(actual, predicted, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    คำนวณ metrics ต่างๆ สำหรับการประเมินโมเดล\n",
        "    \"\"\"\n",
        "    # ตรวจสอบว่าข้อมูลมีค่า NaN หรือไม่\n",
        "    mask = ~(np.isnan(actual) | np.isnan(predicted))\n",
        "    actual_clean = actual[mask]\n",
        "    predicted_clean = predicted[mask]\n",
        "\n",
        "    if len(actual_clean) == 0:\n",
        "        return {}\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'MSE': mean_squared_error(actual_clean, predicted_clean),\n",
        "        'RMSE': np.sqrt(mean_squared_error(actual_clean, predicted_clean)),\n",
        "        'MAE': mean_absolute_error(actual_clean, predicted_clean),\n",
        "        'MAPE': safe_mape(actual_clean, predicted_clean),  # ใช้ safe_mape แทน\n",
        "        'R²': r2_score(actual_clean, predicted_clean),\n",
        "        'Count': len(actual_clean)\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def comprehensive_model_evaluation(df, lstm_predictions=None, lstm_actuals=None,\n",
        "                                 target_column='hotspot_count', siteName='Study Area'):\n",
        "    \"\"\"\n",
        "    การประเมินแบบจำลองแบบครอบคลุมพร้อม baseline models\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"การประเมินแบบจำลองแบบครอบคลุม (Comprehensive Model Evaluation)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # สร้าง baseline models\n",
        "    baseline = BaselineModels(df, target_column)\n",
        "    results = {}\n",
        "    prediction_intervals = {}\n",
        "\n",
        "    print(\"\\n1. การประเมิน Baseline Models\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 1. Seasonal Naïve\n",
        "    print(\"กำลังประเมิน Seasonal Naïve...\")\n",
        "    pred_sn, actual_sn, dates_sn = baseline.seasonal_naive()\n",
        "    if pred_sn is not None:\n",
        "        results['Seasonal_Naive'] = calculate_metrics(actual_sn, pred_sn, \"Seasonal Naïve\")\n",
        "        results['Seasonal_Naive']['predictions'] = pred_sn\n",
        "        results['Seasonal_Naive']['actuals'] = actual_sn\n",
        "        results['Seasonal_Naive']['dates'] = dates_sn\n",
        "\n",
        "    # 2. Naïve with Drift\n",
        "    print(\"กำลังประเมิน Naïve with Drift...\")\n",
        "    pred_nd, actual_nd, dates_nd = baseline.naive_drift()\n",
        "    if pred_nd is not None:\n",
        "        results['Naive_Drift'] = calculate_metrics(actual_nd, pred_nd, \"Naïve with Drift\")\n",
        "        results['Naive_Drift']['predictions'] = pred_nd\n",
        "        results['Naive_Drift']['actuals'] = actual_nd\n",
        "        results['Naive_Drift']['dates'] = dates_nd\n",
        "\n",
        "    # 3. Moving Average\n",
        "    print(\"กำลังประเมิน Moving Average...\")\n",
        "    pred_ma, actual_ma, dates_ma = baseline.moving_average(window=6)\n",
        "    if pred_ma is not None:\n",
        "        results['Moving_Average'] = calculate_metrics(actual_ma, pred_ma, \"Moving Average (6M)\")\n",
        "        results['Moving_Average']['predictions'] = pred_ma\n",
        "        results['Moving_Average']['actuals'] = actual_ma\n",
        "        results['Moving_Average']['dates'] = dates_ma\n",
        "\n",
        "    # 4. Exponential Smoothing (ETS)\n",
        "    print(\"กำลังประเมิน Exponential Smoothing...\")\n",
        "    ets_result = baseline.exponential_smoothing()\n",
        "    if ets_result[0] is not None:\n",
        "        pred_ets, actual_ets, dates_ets = ets_result[:3]\n",
        "        pred_int_ets = ets_result[3] if len(ets_result) > 3 else None\n",
        "\n",
        "        results['ETS'] = calculate_metrics(actual_ets, pred_ets, \"ETS (Holt-Winters)\")\n",
        "        results['ETS']['predictions'] = pred_ets\n",
        "        results['ETS']['actuals'] = actual_ets\n",
        "        results['ETS']['dates'] = dates_ets\n",
        "        if pred_int_ets is not None:\n",
        "            prediction_intervals['ETS'] = pred_int_ets\n",
        "\n",
        "    # 5. ARIMA\n",
        "    print(\"กำลังประเมิน ARIMA...\")\n",
        "    arima_result = baseline.arima_model(order=(2,1,2))\n",
        "    if arima_result[0] is not None:\n",
        "        pred_arima, actual_arima, dates_arima = arima_result[:3]\n",
        "        pred_int_arima = arima_result[3] if len(arima_result) > 3 else None\n",
        "\n",
        "        results['ARIMA'] = calculate_metrics(actual_arima, pred_arima, \"ARIMA(2,1,2)\")\n",
        "        results['ARIMA']['predictions'] = pred_arima\n",
        "        results['ARIMA']['actuals'] = actual_arima\n",
        "        results['ARIMA']['dates'] = dates_arima\n",
        "        if pred_int_arima is not None:\n",
        "            prediction_intervals['ARIMA'] = pred_int_arima\n",
        "\n",
        "    # 6. SARIMA\n",
        "    print(\"กำลังประเมิน SARIMA...\")\n",
        "    sarima_result = baseline.sarima_model(order=(1,1,1), seasonal_order=(1,1,1,12))\n",
        "    if sarima_result[0] is not None:\n",
        "        pred_sarima, actual_sarima, dates_sarima = sarima_result[:3]\n",
        "        pred_int_sarima = sarima_result[3] if len(sarima_result) > 3 else None\n",
        "\n",
        "        results['SARIMA'] = calculate_metrics(actual_sarima, pred_sarima, \"SARIMA(1,1,1)(1,1,1,12)\")\n",
        "        results['SARIMA']['predictions'] = pred_sarima\n",
        "        results['SARIMA']['actuals'] = actual_sarima\n",
        "        results['SARIMA']['dates'] = dates_sarima\n",
        "        if pred_int_sarima is not None:\n",
        "            prediction_intervals['SARIMA'] = pred_int_sarima\n",
        "\n",
        "    # 7. Auto-ARIMA\n",
        "    print(\"กำลังประเมิน Auto-ARIMA...\")\n",
        "    auto_arima_result = baseline.auto_arima()\n",
        "    if auto_arima_result[0] is not None:\n",
        "        pred_auto, actual_auto, dates_auto = auto_arima_result[:3]\n",
        "        pred_int_auto = auto_arima_result[3] if len(auto_arima_result) > 3 else None\n",
        "\n",
        "        results['Auto_ARIMA'] = calculate_metrics(actual_auto, pred_auto, \"Auto-ARIMA\")\n",
        "        results['Auto_ARIMA']['predictions'] = pred_auto\n",
        "        results['Auto_ARIMA']['actuals'] = actual_auto\n",
        "        results['Auto_ARIMA']['dates'] = dates_auto\n",
        "        if pred_int_auto is not None:\n",
        "            prediction_intervals['Auto_ARIMA'] = pred_int_auto\n",
        "\n",
        "    # 8. เพิ่มผล LSTM หากมี\n",
        "    if lstm_predictions is not None and lstm_actuals is not None:\n",
        "        print(\"เพิ่มผลการประเมิน LSTM...\")\n",
        "        results['LSTM'] = calculate_metrics(lstm_actuals, lstm_predictions, \"LSTM\")\n",
        "        results['LSTM']['predictions'] = lstm_predictions\n",
        "        results['LSTM']['actuals'] = lstm_actuals\n",
        "\n",
        "    return results, prediction_intervals\n",
        "\n",
        "def plot_model_comparison_comprehensive(results, prediction_intervals, siteName='Study Area'):\n",
        "    \"\"\"\n",
        "    สร้างแผนภูมิเปรียบเทียบโมเดลแบบครอบคลุม\n",
        "    \"\"\"\n",
        "    # 1. Metrics Comparison\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    models = list(results.keys())\n",
        "    metrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'R²']\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i//3, i%3]\n",
        "        values = [results[model].get(metric, 0) for model in models]\n",
        "\n",
        "        bars = ax.bar(models, values, color=colors)\n",
        "        ax.set_title(f'{metric} Comparison', fontweight='bold')\n",
        "        ax.set_ylabel(metric)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # เพิ่มค่าบน bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            if not np.isnan(value) and value != 0:\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
        "                       f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Model Ranking Table\n",
        "    axes[1, 2].axis('off')\n",
        "    ranking_data = []\n",
        "    for model in models:\n",
        "        if model in results and 'MSE' in results[model]:\n",
        "            ranking_data.append([\n",
        "                results[model]['Model'],\n",
        "                f\"{results[model]['MSE']:.3f}\",\n",
        "                f\"{results[model]['RMSE']:.3f}\",\n",
        "                f\"{results[model]['MAE']:.3f}\",\n",
        "                f\"{results[model]['R²']:.3f}\"\n",
        "            ])\n",
        "\n",
        "    if ranking_data:\n",
        "        table = axes[1, 2].table(\n",
        "            cellText=ranking_data,\n",
        "            colLabels=['Model', 'MSE', 'RMSE', 'MAE', 'R²'],\n",
        "            cellLoc='center',\n",
        "            loc='center'\n",
        "        )\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(9)\n",
        "        table.scale(1.2, 1.5)\n",
        "        axes[1, 2].set_title('Model Performance Summary', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # plt.savefig(f'{siteName}_model_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Predictions vs Actuals with Intervals\n",
        "    if len([m for m in results.keys() if 'predictions' in results[m]]) > 0:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        plot_idx = 0\n",
        "        for model_name in ['Seasonal_Naive', 'ETS', 'SARIMA', 'LSTM']:\n",
        "            if model_name in results and 'predictions' in results[model_name]:\n",
        "                ax = axes[plot_idx]\n",
        "\n",
        "                actuals = results[model_name]['actuals']\n",
        "                predictions = results[model_name]['predictions']\n",
        "\n",
        "                ax.plot(actuals, label='Actual', color='blue', linewidth=2)\n",
        "                ax.plot(predictions, label='Predicted', color='red', linewidth=2, linestyle='--')\n",
        "\n",
        "                # เพิ่ม prediction intervals ถ้ามี\n",
        "                if model_name in prediction_intervals:\n",
        "                    intervals = prediction_intervals[model_name]\n",
        "                    if hasattr(intervals, 'iloc'):  # DataFrame\n",
        "                        lower = intervals.iloc[:, 0].values\n",
        "                        upper = intervals.iloc[:, 1].values\n",
        "                    else:  # Array\n",
        "                        lower = intervals[:, 0]\n",
        "                        upper = intervals[:, 1]\n",
        "\n",
        "                    ax.fill_between(range(len(predictions)), lower, upper,\n",
        "                                   alpha=0.3, color='red', label='95% Confidence Interval')\n",
        "\n",
        "                r2 = results[model_name].get('R²', 0)\n",
        "                rmse = results[model_name].get('RMSE', 0)\n",
        "\n",
        "                ax.set_title(f'{results[model_name][\"Model\"]}\\nR² = {r2:.3f}, RMSE = {rmse:.3f}',\n",
        "                           fontweight='bold')\n",
        "                ax.set_xlabel('Time Period')\n",
        "                ax.set_ylabel('Hotspot Count')\n",
        "                ax.legend()\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "                plot_idx += 1\n",
        "\n",
        "        # ซ่อน subplot ที่ไม่ใช้\n",
        "        for i in range(plot_idx, 4):\n",
        "            axes[i].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # plt.savefig(f'{siteName}_predictions_with_intervals.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "def print_model_summary(results):\n",
        "    \"\"\"\n",
        "    พิมพ์สรุปผลการประเมินโมเดล\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"สรุปผลการประเมินโมเดล (Model Evaluation Summary)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # สร้างตาราง\n",
        "    summary_data = []\n",
        "    for model_name, metrics in results.items():\n",
        "        if 'MSE' in metrics:\n",
        "            summary_data.append([\n",
        "                metrics['Model'],\n",
        "                f\"{metrics['MSE']:.4f}\",\n",
        "                f\"{metrics['RMSE']:.4f}\",\n",
        "                f\"{metrics['MAE']:.4f}\",\n",
        "                f\"{metrics['MAPE']:.2f}%\",\n",
        "                f\"{metrics['R²']:.4f}\"\n",
        "            ])\n",
        "\n",
        "    # เรียงลำดับตาม RMSE\n",
        "    summary_data.sort(key=lambda x: float(x[2]))\n",
        "\n",
        "    print(f\"{'Rank':<4} {'Model':<20} {'MSE':<10} {'RMSE':<10} {'MAE':<10} {'MAPE':<10} {'R²':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i, row in enumerate(summary_data):\n",
        "        print(f\"{i+1:<4} {row[0]:<20} {row[1]:<10} {row[2]:<10} {row[3]:<10} {row[4]:<10} {row[5]:<10}\")\n",
        "\n",
        "    # แสดงโมเดลที่ดีที่สุด\n",
        "    if summary_data:\n",
        "        best_model = summary_data[0][0]\n",
        "        print(f\"\\n🏆 โมเดลที่ดีที่สุด (ตาม RMSE): {best_model}\")\n",
        "\n",
        "        # วิเคราะห์การปรับปรุง\n",
        "        if 'LSTM' in [row[0] for row in summary_data]:\n",
        "            lstm_rank = next(i for i, row in enumerate(summary_data) if row[0] == 'LSTM') + 1\n",
        "            print(f\"📊 อันดับของ LSTM: {lstm_rank} จาก {len(summary_data)} โมเดล\")\n",
        "\n",
        "            if lstm_rank == 1:\n",
        "                print(\"✅ LSTM มีประสิทธิภาพดีที่สุด\")\n",
        "            else:\n",
        "                print(f\"⚠️  LSTM อันดับ {lstm_rank} ควรพิจารณาปรับปรุงเพิ่มเติม\")\n",
        "\n",
        "# ฟังก์ชันสร้างกราฟทำนาย 12 เดือน - Updated for hotspot_count with full historical data\n",
        "def plot_12_month_prediction(df, predictions, prediction_dates, target_column='hotspot_count'):\n",
        "    \"\"\"แสดงกราฟข้อมูลจริงและการทำนาย 12 เดือน\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        # ใช้ style ที่สวยงาม\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 9))\n",
        "\n",
        "        # กราฟที่ 1: ข้อมูลทั้งหมดตั้งแต่ 2018 + การทำนาย\n",
        "        # แสดงข้อมูลทั้งหมดตั้งแต่ 2018-01-01\n",
        "        ax1.plot(df['date'], df[target_column],\n",
        "                label=f'Historical {target_column} (2018-present)',\n",
        "                color='blue', marker='o', markersize=3, linewidth=1.5, alpha=0.8)\n",
        "\n",
        "        # แสดงข้อมูล 6 เดือนล่าสุดที่ใช้ทำนาย (highlight)\n",
        "        last_6_months = df.tail(6)\n",
        "        ax1.plot(last_6_months['date'], last_6_months[target_column],\n",
        "                label=f'Last 6 months (prediction input)',\n",
        "                color='orange', marker='s', markersize=6, linewidth=3, alpha=1.0)\n",
        "\n",
        "        # แสดงการทำนาย 12 เดือน\n",
        "        ax1.plot(prediction_dates, predictions,\n",
        "                label=f'12-month predictions', color='red', marker='*', markersize=8, linewidth=3)\n",
        "\n",
        "        # เชื่อมเส้นจากข้อมูลล่าสุดไปยังการทำนาย\n",
        "        ax1.plot([df['date'].iloc[-1], prediction_dates[0]],\n",
        "                [df[target_column].iloc[-1], predictions[0]],\n",
        "                '--', color='red', alpha=0.7, linewidth=2)\n",
        "\n",
        "        # เพิ่มข้อมูลสถิติบนกราฟ\n",
        "        historical_mean = df[target_column].mean()\n",
        "        historical_max = df[target_column].max()\n",
        "        historical_min = df[target_column].min()\n",
        "\n",
        "        ax1.axhline(y=historical_mean, color='gray', linestyle='--', alpha=0.7,\n",
        "                   label=f'Historical Mean ({historical_mean:.1f})')\n",
        "\n",
        "        ax1.set_title(f'{target_column} - Complete Historical Data (2018-present) & 12-Month Predictions ({siteName})',\n",
        "                     fontsize=16, fontweight='bold')\n",
        "        ax1.set_xlabel('Date', fontsize=12)\n",
        "        ax1.set_ylabel(f'{target_column} Value', fontsize=12)\n",
        "        ax1.legend(fontsize=10, loc='upper left')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # ปรับการแสดงวันที่ให้เหมาะกับข้อมูลหลายปี\n",
        "        ax1.tick_params(axis='x', rotation=45, labelsize=10)\n",
        "\n",
        "        # เพิ่ม text box แสดงสถิติ\n",
        "        stats_text = f'Historical Stats (2018-present):\\nMean: {historical_mean:.1f}\\nMax: {historical_max:.1f}\\nMin: {historical_min:.1f}\\nData Points: {len(df)}'\n",
        "        # ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, fontsize=10,\n",
        "        #         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "        # กราฟที่ 2: Seasonal Pattern with Enhanced Information\n",
        "        months = [date.month for date in prediction_dates]\n",
        "        month_names = [calendar.month_name[month][:3] for month in months]\n",
        "\n",
        "        # Update color thresholds for hotspot count based on historical data\n",
        "        high_threshold = df[target_column].quantile(0.75)\n",
        "        medium_threshold = df[target_column].quantile(0.5)\n",
        "\n",
        "        colors = ['red' if pred > high_threshold else\n",
        "                 'orange' if pred > medium_threshold else 'green'\n",
        "                 for pred in predictions]\n",
        "\n",
        "        bars = ax2.bar(month_names, predictions, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
        "\n",
        "        # Add historical monthly averages for comparison\n",
        "        historical_monthly = df.groupby('month')[target_column].mean()\n",
        "        month_labels = [calendar.month_name[i][:3] for i in range(1, 13)]\n",
        "        historical_values = [historical_monthly.get(i, 0) for i in range(1, 13)]\n",
        "\n",
        "        # Show only the months we're predicting for comparison\n",
        "        pred_months = [date.month for date in prediction_dates]\n",
        "        historical_comparison = [historical_monthly.get(month, 0) for month in pred_months]\n",
        "\n",
        "        ax2_twin = ax2.twinx()\n",
        "        ax2_twin.plot(month_names, historical_comparison, 'k--', marker='d',\n",
        "                     linewidth=2, markersize=6, label='Historical Monthly Average', alpha=0.8)\n",
        "        ax2_twin.set_ylabel('Historical Average', fontsize=12, color='black')\n",
        "        ax2_twin.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "        ax2.set_title(f'Monthly Hotspot Count Predictions vs Historical Averages (Next 12 Months) ({siteName})',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('Month', fontsize=12)\n",
        "        ax2.set_ylabel(f'Predicted {target_column}', fontsize=12)\n",
        "        ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # เพิ่มค่าบนแท่งกราฟ\n",
        "        for bar, pred in zip(bars, predictions):\n",
        "            height = bar.get_height()\n",
        "            ax2.annotate(f'{pred:.0f}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "        # เพิ่ม legend สำหรับสี\n",
        "        import matplotlib.patches as mpatches\n",
        "        high_patch = mpatches.Patch(color='red', alpha=0.7, label=f'High Risk (>{high_threshold:.0f})')\n",
        "        medium_patch = mpatches.Patch(color='orange', alpha=0.7, label=f'Medium Risk ({medium_threshold:.0f}-{high_threshold:.0f})')\n",
        "        low_patch = mpatches.Patch(color='green', alpha=0.7, label=f'Low Risk (<{medium_threshold:.0f})')\n",
        "\n",
        "        # Combine legends from both axes\n",
        "        bars_legend = [high_patch, medium_patch, low_patch]\n",
        "        line_legend = ax2_twin.get_legend_handles_labels()\n",
        "\n",
        "        ax2.legend(handles=bars_legend, loc='upper left')\n",
        "        ax2_twin.legend(loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # plt.savefig(f'{siteName}_12_month_prediction.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"บันทึกกราฟทำนาย 12 เดือน: {siteName}_12_month_prediction.png\")\n",
        "\n",
        "        # Print summary of changes from historical averages\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"การเปรียบเทียบการทำนายกับค่าเฉลี่ยในอดีต:\")\n",
        "        print(\"=\"*60)\n",
        "        for i, (month_name, pred, hist) in enumerate(zip(month_names, predictions, historical_comparison)):\n",
        "            diff = pred - hist\n",
        "            change_pct = (diff / hist * 100) if hist > 0 else 0\n",
        "            status = \"↑ เพิ่มขึ้น\" if diff > 0 else \"↓ ลดลง\" if diff < 0 else \"→ ไม่เปลี่ยนแปลง\"\n",
        "            print(f\"{month_name}: ทำนาย {pred:.0f}, เฉลี่ยในอดีต {hist:.1f}, ต่าง {diff:+.1f} ({change_pct:+.1f}%) {status}\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ไม่สามารถ import matplotlib หรือ seaborn ได้\")\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้างกราฟทำนาย 12 เดือน: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8520ee2",
      "metadata": {
        "id": "d8520ee2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LSTM Model Save/Load System for GEE Hotspot Prediction\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import joblib\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "\n",
        "class HotspotLSTMPredictor:\n",
        "    \"\"\"\n",
        "    Simple class to save, load, and use LSTM model for hotspot prediction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dir=\"saved_models\"):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.scaler_x = None\n",
        "        self.scaler_y = None\n",
        "        self.sequence_length = 6\n",
        "        self.feature_columns = [\n",
        "            'NDVI', 'month_sin', 'month_cos', 'season_sin', 'season_cos',\n",
        "            'summer_season', 'rainy_season', 'winter_season',\n",
        "            'peak_fire_season', 'low_fire_season'\n",
        "        ]\n",
        "\n",
        "        # Create model directory if it doesn't exist\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.makedirs(self.model_dir)\n",
        "\n",
        "    def save_model(self, model, scaler_x, scaler_y, model_name=\"hotspot_lstm\"):\n",
        "        \"\"\"\n",
        "        Save trained model and scalers\n",
        "\n",
        "        Args:\n",
        "            model: Trained Keras model\n",
        "            scaler_x: Fitted StandardScaler for features\n",
        "            scaler_y: Fitted StandardScaler for target\n",
        "            model_name: Name for saved files\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Save model using Keras native format (.keras)\n",
        "            model_path = os.path.join(self.model_dir, f\"{model_name}.keras\")\n",
        "            model.save(model_path)\n",
        "            print(f\"✅ Model saved (.keras format): {model_path}\")\n",
        "\n",
        "            # Also save model weights separately as backup\n",
        "            weights_path = os.path.join(self.model_dir, f\"{model_name}.weights.h5\")\n",
        "            model.save_weights(weights_path)\n",
        "            print(f\"✅ Model weights saved: {weights_path}\")\n",
        "\n",
        "            # Save model architecture\n",
        "            architecture_path = os.path.join(self.model_dir, f\"{model_name}_architecture.json\")\n",
        "            with open(architecture_path, 'w') as f:\n",
        "                f.write(model.to_json())\n",
        "            print(f\"✅ Model architecture saved: {architecture_path}\")\n",
        "\n",
        "            # Save scalers\n",
        "            scaler_x_path = os.path.join(self.model_dir, f\"{model_name}_scaler_x.pkl\")\n",
        "            scaler_y_path = os.path.join(self.model_dir, f\"{model_name}_scaler_y.pkl\")\n",
        "\n",
        "            joblib.dump(scaler_x, scaler_x_path)\n",
        "            joblib.dump(scaler_y, scaler_y_path)\n",
        "            print(f\"✅ Scalers saved: {scaler_x_path}, {scaler_y_path}\")\n",
        "\n",
        "            # Save model metadata\n",
        "            metadata = {\n",
        "                'sequence_length': self.sequence_length,\n",
        "                'feature_columns': self.feature_columns,\n",
        "                'model_name': model_name,\n",
        "                'save_date': datetime.now().isoformat(),\n",
        "                'input_shape': model.input_shape,\n",
        "                'output_shape': model.output_shape,\n",
        "                'model_config': model.get_config()\n",
        "            }\n",
        "\n",
        "            metadata_path = os.path.join(self.model_dir, f\"{model_name}_metadata.pkl\")\n",
        "            with open(metadata_path, 'wb') as f:\n",
        "                pickle.dump(metadata, f)\n",
        "            print(f\"✅ Metadata saved: {metadata_path}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_model(self, model_name=\"hotspot_lstm\"):\n",
        "        \"\"\"\n",
        "        Load saved model and scalers with multiple fallback options\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of saved model files\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Method 1: Try loading .keras format first\n",
        "            model_path = os.path.join(self.model_dir, f\"{model_name}.keras\")\n",
        "            if os.path.exists(model_path):\n",
        "                try:\n",
        "                    from tensorflow.keras.models import load_model\n",
        "                    # Custom objects to handle potential deserialization issues\n",
        "                    custom_objects = {\n",
        "                        'mse': 'mse',\n",
        "                        'mae': 'mae',\n",
        "                        'mean_squared_error': 'mean_squared_error',\n",
        "                        'mean_absolute_error': 'mean_absolute_error'\n",
        "                    }\n",
        "\n",
        "                    self.model = load_model(model_path, custom_objects=custom_objects)\n",
        "                    print(f\"✅ Model loaded (.keras format): {model_path}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ SavedModel loading failed: {e}\")\n",
        "                    print(\"Trying alternative method...\")\n",
        "                    raise e\n",
        "            else:\n",
        "                raise FileNotFoundError(\"SavedModel not found, trying alternative method...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Method 2: Try loading from architecture + weights\n",
        "            try:\n",
        "                from tensorflow.keras.models import model_from_json\n",
        "                from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "                # Load architecture\n",
        "                architecture_path = os.path.join(self.model_dir, f\"{model_name}_architecture.json\")\n",
        "                with open(architecture_path, 'r') as f:\n",
        "                    model_json = f.read()\n",
        "\n",
        "                self.model = model_from_json(model_json)\n",
        "                print(f\"✅ Model architecture loaded: {architecture_path}\")\n",
        "\n",
        "                # Load weights\n",
        "                weights_path = os.path.join(self.model_dir, f\"{model_name}.weights.h5\")\n",
        "                self.model.load_weights(weights_path)\n",
        "                print(f\"✅ Model weights loaded: {weights_path}\")\n",
        "\n",
        "                # Recompile the model to avoid metric issues\n",
        "                self.model.compile(\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='mse',\n",
        "                    metrics=['mae']\n",
        "                )\n",
        "                print(\"✅ Model recompiled successfully\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                # Method 3: Try legacy H5 format with custom objects\n",
        "                try:\n",
        "                    from tensorflow.keras.models import load_model\n",
        "                    import tensorflow.keras.utils as utils\n",
        "\n",
        "                    # Try H5 format with custom objects\n",
        "                    h5_model_path = os.path.join(self.model_dir, f\"{model_name}.h5\")\n",
        "\n",
        "                    # Create custom objects mapping\n",
        "                    custom_objects = {\n",
        "                        'mse': utils.get_custom_objects().get('mse', 'mse'),\n",
        "                        'mae': utils.get_custom_objects().get('mae', 'mae'),\n",
        "                    }\n",
        "\n",
        "                    self.model = load_model(h5_model_path, custom_objects=custom_objects, compile=False)\n",
        "\n",
        "                    # Recompile manually\n",
        "                    self.model.compile(\n",
        "                        optimizer=Adam(learning_rate=0.001),\n",
        "                        loss='mse',\n",
        "                        metrics=['mae']\n",
        "                    )\n",
        "                    print(f\"✅ Model loaded (H5 format with recompilation): {h5_model_path}\")\n",
        "\n",
        "                except Exception as e3:\n",
        "                    print(f\"❌ All loading methods failed:\")\n",
        "                    print(f\"  SavedModel error: {e}\")\n",
        "                    print(f\"  Architecture+Weights error: {e2}\")\n",
        "                    print(f\"  H5 format error: {e3}\")\n",
        "                    return False\n",
        "\n",
        "        try:\n",
        "            # Load scalers\n",
        "            scaler_x_path = os.path.join(self.model_dir, f\"{model_name}_scaler_x.pkl\")\n",
        "            scaler_y_path = os.path.join(self.model_dir, f\"{model_name}_scaler_y.pkl\")\n",
        "\n",
        "            self.scaler_x = joblib.load(scaler_x_path)\n",
        "            self.scaler_y = joblib.load(scaler_y_path)\n",
        "            print(f\"✅ Scalers loaded\")\n",
        "\n",
        "            # Load metadata\n",
        "            metadata_path = os.path.join(self.model_dir, f\"{model_name}_metadata.pkl\")\n",
        "            with open(metadata_path, 'rb') as f:\n",
        "                metadata = pickle.load(f)\n",
        "\n",
        "            self.sequence_length = metadata['sequence_length']\n",
        "            self.feature_columns = metadata['feature_columns']\n",
        "            print(f\"✅ Metadata loaded - Sequence length: {self.sequence_length}\")\n",
        "\n",
        "            # Test the model with a dummy prediction to ensure it works\n",
        "            dummy_input = np.random.random((1, self.sequence_length, len(self.feature_columns)))\n",
        "            try:\n",
        "                _ = self.model.predict(dummy_input, verbose=0)\n",
        "                print(\"✅ Model validation successful - ready for predictions\")\n",
        "            except Exception as test_error:\n",
        "                print(f\"⚠️ Model loaded but validation failed: {test_error}\")\n",
        "                print(\"Model may still work for predictions, proceeding...\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading scalers or metadata: {e}\")\n",
        "            return False\n",
        "\n",
        "    def prepare_new_data(self, df):\n",
        "        \"\"\"\n",
        "        Prepare new GEE data for prediction (same preprocessing as training)\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with columns ['date', 'NDVI', 'hotspot_count', 'month', 'year']\n",
        "\n",
        "        Returns:\n",
        "            Prepared data ready for prediction\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Add seasonal features (same as training)\n",
        "            df_prep = df.copy()\n",
        "\n",
        "            # Monthly cyclical encoding\n",
        "            df_prep['month_sin'] = np.sin(2 * np.pi * df_prep['month'] / 12)\n",
        "            df_prep['month_cos'] = np.cos(2 * np.pi * df_prep['month'] / 12)\n",
        "\n",
        "            # Asian seasonal features\n",
        "            def get_asian_season_number(month):\n",
        "                if month in [3, 4, 5]:\n",
        "                    return 1  # Summer\n",
        "                elif month in [6, 7, 8, 9, 10]:\n",
        "                    return 2  # Rainy\n",
        "                else:\n",
        "                    return 3  # Winter\n",
        "\n",
        "            df_prep['season_number'] = df_prep['month'].apply(get_asian_season_number)\n",
        "            df_prep['season_sin'] = np.sin(2 * np.pi * df_prep['season_number'] / 3)\n",
        "            df_prep['season_cos'] = np.cos(2 * np.pi * df_prep['season_number'] / 3)\n",
        "\n",
        "            # Binary seasonal encoding\n",
        "            df_prep['summer_season'] = (df_prep['month'].isin([3, 4, 5])).astype(int)\n",
        "            df_prep['rainy_season'] = (df_prep['month'].isin([6, 7, 8, 9, 10])).astype(int)\n",
        "            df_prep['winter_season'] = (df_prep['month'].isin([11, 12, 1, 2])).astype(int)\n",
        "\n",
        "            # Fire season indicators\n",
        "            df_prep['peak_fire_season'] = (df_prep['month'].isin([2, 3, 4])).astype(int)\n",
        "            df_prep['low_fire_season'] = (df_prep['month'].isin([6, 7, 8, 9])).astype(int)\n",
        "\n",
        "            # Select feature columns\n",
        "            X = df_prep[self.feature_columns].values\n",
        "\n",
        "            # Scale features\n",
        "            X_scaled = self.scaler_x.transform(X)\n",
        "\n",
        "            print(f\"✅ Data prepared: {X_scaled.shape}\")\n",
        "            return X_scaled, df_prep\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error preparing data: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def create_sequences(self, X_scaled):\n",
        "        \"\"\"\n",
        "        Create sequences for LSTM prediction\n",
        "\n",
        "        Args:\n",
        "            X_scaled: Scaled feature data\n",
        "\n",
        "        Returns:\n",
        "            Sequences ready for LSTM\n",
        "        \"\"\"\n",
        "        if len(X_scaled) < self.sequence_length:\n",
        "            print(f\"❌ Need at least {self.sequence_length} data points, got {len(X_scaled)}\")\n",
        "            return None\n",
        "\n",
        "        # Create sequences\n",
        "        X_seq = []\n",
        "        for i in range(len(X_scaled) - self.sequence_length + 1):\n",
        "            X_seq.append(X_scaled[i:i+self.sequence_length])\n",
        "\n",
        "        return np.array(X_seq)\n",
        "\n",
        "    def predict_new_data(self, df):\n",
        "        \"\"\"\n",
        "        Predict hotspot counts for new GEE data\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with GEE data\n",
        "\n",
        "        Returns:\n",
        "            Predictions and dates\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"❌ Model not loaded. Call load_model() first.\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            # Prepare data\n",
        "            X_scaled, df_prep = self.prepare_new_data(df)\n",
        "            if X_scaled is None:\n",
        "                return None, None\n",
        "\n",
        "            # Create sequences\n",
        "            X_sequences = self.create_sequences(X_scaled)\n",
        "            if X_sequences is None:\n",
        "                return None, None\n",
        "\n",
        "            # Make predictions\n",
        "            predictions_scaled = self.model.predict(X_sequences, verbose=0)\n",
        "\n",
        "            # Inverse transform predictions\n",
        "            predictions = self.scaler_y.inverse_transform(predictions_scaled).flatten()\n",
        "\n",
        "            # Get corresponding dates\n",
        "            prediction_dates = df_prep['date'].iloc[self.sequence_length-1:].values\n",
        "\n",
        "            print(f\"✅ Predictions completed: {len(predictions)} values\")\n",
        "            return predictions, prediction_dates\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in prediction: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def predict_future_months(self, df, months_ahead=12):\n",
        "        \"\"\"\n",
        "        Predict future hotspot counts\n",
        "\n",
        "        Args:\n",
        "            df: Historical GEE data (last sequence_length months will be used)\n",
        "            months_ahead: Number of months to predict\n",
        "\n",
        "        Returns:\n",
        "            Future predictions and dates\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"❌ Model not loaded. Call load_model() first.\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            # Use last sequence_length data points\n",
        "            df_recent = df.tail(self.sequence_length).copy()\n",
        "\n",
        "            # Prepare initial sequence\n",
        "            X_scaled, df_prep = self.prepare_new_data(df_recent)\n",
        "            if X_scaled is None:\n",
        "                return None, None\n",
        "\n",
        "            # Get last sequence\n",
        "            current_sequence = X_scaled[-self.sequence_length:].reshape(1, self.sequence_length, -1)\n",
        "\n",
        "            predictions = []\n",
        "            prediction_dates = []\n",
        "\n",
        "            # Get last date\n",
        "            last_date = df['date'].iloc[-1]\n",
        "\n",
        "            for i in range(months_ahead):\n",
        "                # Predict next month\n",
        "                pred_scaled = self.model.predict(current_sequence, verbose=0)\n",
        "                pred_original = self.scaler_y.inverse_transform(pred_scaled)[0][0]\n",
        "                predictions.append(pred_original)\n",
        "\n",
        "                # Calculate next date\n",
        "                if last_date.month == 12:\n",
        "                    next_date = datetime(last_date.year + 1, 1, 1)\n",
        "                else:\n",
        "                    next_date = datetime(last_date.year, last_date.month + 1, 1)\n",
        "                prediction_dates.append(next_date)\n",
        "                last_date = next_date\n",
        "\n",
        "                # Create features for next month\n",
        "                next_month = next_date.month\n",
        "\n",
        "                # Calculate features (same logic as training)\n",
        "                month_sin = np.sin(2 * np.pi * next_month / 12)\n",
        "                month_cos = np.cos(2 * np.pi * next_month / 12)\n",
        "\n",
        "                def get_asian_season_number(month):\n",
        "                    if month in [3, 4, 5]:\n",
        "                        return 1\n",
        "                    elif month in [6, 7, 8, 9, 10]:\n",
        "                        return 2\n",
        "                    else:\n",
        "                        return 3\n",
        "\n",
        "                season_number = get_asian_season_number(next_month)\n",
        "                season_sin = np.sin(2 * np.pi * season_number / 3)\n",
        "                season_cos = np.cos(2 * np.pi * season_number / 3)\n",
        "\n",
        "                summer_season = 1 if next_month in [3, 4, 5] else 0\n",
        "                rainy_season = 1 if next_month in [6, 7, 8, 9, 10] else 0\n",
        "                winter_season = 1 if next_month in [11, 12, 1, 2] else 0\n",
        "\n",
        "                peak_fire_season = 1 if next_month in [2, 3, 4] else 0\n",
        "                low_fire_season = 1 if next_month in [6, 7, 8, 9] else 0\n",
        "\n",
        "                # Estimate NDVI (use historical average for same month)\n",
        "                same_month_data = df[df['month'] == next_month]\n",
        "                if not same_month_data.empty:\n",
        "                    avg_ndvi = same_month_data['NDVI'].mean()\n",
        "                else:\n",
        "                    avg_ndvi = df['NDVI'].mean()\n",
        "\n",
        "                # Create feature vector\n",
        "                next_features = np.array([[\n",
        "                    avg_ndvi, month_sin, month_cos, season_sin, season_cos,\n",
        "                    summer_season, rainy_season, winter_season,\n",
        "                    peak_fire_season, low_fire_season\n",
        "                ]])\n",
        "\n",
        "                # Scale features\n",
        "                next_features_scaled = self.scaler_x.transform(next_features)\n",
        "\n",
        "                # Update sequence\n",
        "                current_sequence = np.roll(current_sequence, -1, axis=1)\n",
        "                current_sequence[0, -1] = next_features_scaled[0]\n",
        "\n",
        "            print(f\"✅ Future predictions completed: {len(predictions)} months\")\n",
        "            return predictions, prediction_dates\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in future prediction: {e}\")\n",
        "            return None, None\n",
        "\n",
        "# ============================================================================\n",
        "# Simple Usage Functions\n",
        "# ============================================================================\n",
        "\n",
        "def save_trained_model(model, scaler_x, scaler_y, model_name=\"hotspot_lstm_model\"):\n",
        "    \"\"\"\n",
        "    Simple function to save your trained model\n",
        "\n",
        "    Usage after training:\n",
        "        save_trained_model(model, scaler_x, scaler_y, \"my_hotspot_model\")\n",
        "    \"\"\"\n",
        "    predictor = HotspotLSTMPredictor()\n",
        "    return predictor.save_model(model, scaler_x, scaler_y, model_name)\n",
        "\n",
        "def load_and_predict(new_gee_dataframe, model_name=\"hotspot_lstm_model\", future_months=0):\n",
        "    \"\"\"\n",
        "    Simple function to load model and predict on new GEE data\n",
        "\n",
        "    Args:\n",
        "        new_gee_dataframe: DataFrame with columns ['date', 'NDVI', 'hotspot_count', 'month', 'year']\n",
        "        model_name: Name of saved model\n",
        "        future_months: Number of future months to predict (0 = only predict for existing data)\n",
        "\n",
        "    Returns:\n",
        "        predictions, dates\n",
        "\n",
        "    Usage:\n",
        "        predictions, dates = load_and_predict(new_df, \"my_hotspot_model\", future_months=12)\n",
        "    \"\"\"\n",
        "    predictor = HotspotLSTMPredictor()\n",
        "\n",
        "    # Load model\n",
        "    if not predictor.load_model(model_name):\n",
        "        return None, None\n",
        "\n",
        "    if future_months > 0:\n",
        "        # Predict future\n",
        "        predictions, dates = predictor.predict_future_months(new_gee_dataframe, future_months)\n",
        "    else:\n",
        "        # Predict for existing data\n",
        "        predictions, dates = predictor.predict_new_data(new_gee_dataframe)\n",
        "\n",
        "    return predictions, dates\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0e78d2",
      "metadata": {
        "id": "ed0e78d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Updated main function to use temperature filtering\n",
        "def main():\n",
        "    print(\"เริ่มการวิเคราะห์และทำนาย HOTSPOT COUNT รายเดือนสำหรับ 12 เดือนข้างหน้า (พร้อมการกรอง T21)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # กำหนดช่วงเวลาข้อมูล\n",
        "    start_date = '2017-08-01'\n",
        "    end_date = '2025-08-31'\n",
        "\n",
        "    # พารามิเตอร์สำหรับการกรอง hotspot 300-310k, 310-320k, >320k\n",
        "    TEMP_MIN = 310  # Kelvin - ปรับได้ตามต้องการ\n",
        "    TEMP_MAX = 320  # Kelvin - ปรับได้ตามต้องการ\n",
        "    USE_ALL_HOTSPOTS = False  # True = ใช้ T21 > 0, False = ใช้ช่วงอุณหภูมิ\n",
        "\n",
        "    print(f\"การตั้งค่าการกรอง hotspot:\")\n",
        "    if USE_ALL_HOTSPOTS:\n",
        "        print(\"- ใช้ทุกจุดความร้อน (T21 > 0)\")\n",
        "    else:\n",
        "        print(f\"- ใช้ช่วงอุณหภูมิ: {TEMP_MIN}K ≤ T21 < {TEMP_MAX}K\")\n",
        "    print()\n",
        "\n",
        "    print(\"กำลังดึงข้อมูล NDVI รายเดือนจาก Google Earth Engine...\")\n",
        "    ndvi_data = get_monthly_ndvi(start_date, end_date, study_area)\n",
        "\n",
        "    print(\"กำลังดึงข้อมูล Hotspot รายเดือนพร้อมการกรอง T21...\")\n",
        "    hotspot_data = get_monthly_hotspots(\n",
        "        start_date, end_date, study_area,\n",
        "        temp_min=TEMP_MIN,\n",
        "        temp_max=TEMP_MAX,\n",
        "        all_hotspots=USE_ALL_HOTSPOTS\n",
        "    )\n",
        "\n",
        "    if ndvi_data is not None and hotspot_data is not None:\n",
        "        print(f\"จำนวนเดือนที่มีข้อมูล NDVI: {ndvi_data.size().getInfo()}\")\n",
        "        print(f\"จำนวนเดือนที่มีข้อมูล Hotspot: {hotspot_data.size().getInfo()}\")\n",
        "\n",
        "        print(\"กำลังสร้างชุดข้อมูลรวม...\")\n",
        "        dataset = create_monthly_dataset_with_filtered_hotspots(ndvi_data, hotspot_data, study_area)\n",
        "\n",
        "        if dataset is not None:\n",
        "            print(\"กำลังแปลงข้อมูลเป็น DataFrame...\")\n",
        "            df = fc_to_df(dataset)\n",
        "            print(f\"จำนวนข้อมูลใน DataFrame: {len(df)} เดือน\")\n",
        "\n",
        "            if not df.empty and len(df) > 24:\n",
        "                print(\"ตัวอย่างข้อมูล:\")\n",
        "                print(df.head(10))\n",
        "                print(f\"\\nช่วงข้อมูล: {df['date'].min()} ถึง {df['date'].max()}\")\n",
        "                print(f\"สถิติพื้นฐาน hotspot count:\")\n",
        "                print(f\"- ค่าเฉลี่ย: {df['hotspot_count'].mean():.2f}\")\n",
        "                print(f\"- ค่าสูงสุด: {df['hotspot_count'].max():.0f}\")\n",
        "                print(f\"- ค่าต่ำสุด: {df['hotspot_count'].min():.0f}\")\n",
        "                print(f\"- ส่วนเบียงเบนมาตรฐาน: {df['hotspot_count'].std():.2f}\")\n",
        "\n",
        "                # ส่วนใหม่: การวิเคราะห์ Time Series ขั้นสูง\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"กำลังสร้างกราฟการวิเคราะห์ Time Series ขั้นสูง...\")\n",
        "                print(\"=\"*60)\n",
        "\n",
        "                # 1. Comprehensive time series analysis\n",
        "                plot_ndvi_hotspot_time_series(df)\n",
        "\n",
        "                # 2. Yearly comparison analysis\n",
        "                plot_yearly_comparison(df)\n",
        "\n",
        "                # 3. Statistical analysis report\n",
        "                analysis_results = create_comprehensive_analysis_report(df)\n",
        "\n",
        "                # ส่วนเดิม: การสร้างแบบจำลอง LSTM (ปรับปรุง)\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"กำลังดำเนินการสร้างแบบจำลองและทำนาย...\")\n",
        "                print(\"=\"*60)\n",
        "\n",
        "                print(\"กำลังเตรียมข้อมูลสำหรับการสร้างแบบจำลอง...\")\n",
        "                try:\n",
        "                    X, y, scaler_x, scaler_y, df_cleaned = prepare_monthly_training_data(\n",
        "                        df, sequence_length=6, target_column='hotspot_count'\n",
        "                    )\n",
        "                    print(f\"ข้อมูลที่เตรียม: X.shape = {X.shape}, y.shape = {y.shape}\")\n",
        "\n",
        "                    # แบ่งข้อมูลสำหรับการประเมิน\n",
        "                    split_idx = int(len(X) * 0.8)\n",
        "                    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "                    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "                    print(f\"ข้อมูลสอน: {X_train.shape}, ข้อมูลทดสอบ: {X_test.shape}\")\n",
        "\n",
        "                    # สร้างและสอนแบบจำลอง\n",
        "                    print(\"กำลังสร้างแบบจำลอง LSTM...\")\n",
        "                    model = create_monthly_lstm_model(X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "                    if model is None:\n",
        "                        print(\"ไม่สามารถสร้างแบบจำลองได้ กรุณาตรวจสอบการติดตั้ง TensorFlow\")\n",
        "                        return\n",
        "\n",
        "                    print(\"กำลังสอนแบบจำลอง...\")\n",
        "\n",
        "                    # เพิ่ม callbacks สำหรับการสอนที่ดีขึ้น\n",
        "                    try:\n",
        "                        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "                        early_stopping = EarlyStopping(\n",
        "                            monitor='val_loss',\n",
        "                            patience=15,\n",
        "                            restore_best_weights=True,\n",
        "                            verbose=1\n",
        "                        )\n",
        "\n",
        "                        reduce_lr = ReduceLROnPlateau(\n",
        "                            monitor='val_loss',\n",
        "                            factor=0.5,\n",
        "                            patience=10,\n",
        "                            min_lr=1e-7,\n",
        "                            verbose=1\n",
        "                        )\n",
        "\n",
        "                        callbacks = [early_stopping, reduce_lr]\n",
        "                    except ImportError:\n",
        "                        callbacks = []\n",
        "\n",
        "                    history = model.fit(\n",
        "                        X_train, y_train,\n",
        "                        epochs=100,\n",
        "                        batch_size=32,\n",
        "                        validation_data=(X_test, y_test) if len(X_test) > 0 else None,\n",
        "                        callbacks=callbacks,\n",
        "                        verbose=0\n",
        "                    )\n",
        "\n",
        "                    print(\"Saving trained model...\")\n",
        "                    predictor = HotspotLSTMPredictor()\n",
        "                    success = predictor.save_model(model, scaler_x, scaler_y, f\"{siteName}_hotspot_model\")\n",
        "\n",
        "                    if success:\n",
        "                        print(f\"✅ Model saved successfully for {siteName}\")\n",
        "\n",
        "                    # ส่วนเดิม: กราฟ Learning Curve\n",
        "                    print(\"\\nกำลังสร้างกราฟ Learning Curve...\")\n",
        "                    plot_learning_curve(history)\n",
        "\n",
        "                    # ส่วนใหม่: การประเมินแบบครอบคลุม\n",
        "                    if len(X_test) > 0:\n",
        "                        print(\"\\n\" + \"=\"*60)\n",
        "                        print(\"กำลังดำเนินการประเมินแบบจำลองแบบครอบคลุม...\")\n",
        "                        print(\"=\"*60)\n",
        "\n",
        "                        # ทำนายด้วย LSTM\n",
        "                        y_pred = model.predict(X_test)\n",
        "                        y_pred_rescaled = scaler_y.inverse_transform(y_pred).flatten()\n",
        "                        y_test_rescaled = scaler_y.inverse_transform(y_test).flatten()\n",
        "\n",
        "                        print(\"ผลการประเมิน LSTM (เบื้องต้น):\")\n",
        "                        lstm_mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
        "                        lstm_rmse = np.sqrt(lstm_mse)\n",
        "                        lstm_mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
        "                        lstm_r2 = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "\n",
        "                        print(f\"LSTM - RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, R²: {lstm_r2:.4f}\")\n",
        "\n",
        "                        # การประเมินแบบครอบคลุมใหม่\n",
        "                        print(\"\\nเริ่มการประเมินแบบครอบคลุมกับ Baseline Models...\")\n",
        "\n",
        "                        # เรียกใช้ฟังก์ชันประเมินแบบครอบคลุม\n",
        "                        results, prediction_intervals = comprehensive_model_evaluation(\n",
        "                            df=df_cleaned,\n",
        "                            lstm_predictions=y_pred_rescaled,\n",
        "                            lstm_actuals=y_test_rescaled,\n",
        "                            target_column='hotspot_count',\n",
        "                            siteName=siteName\n",
        "                        )\n",
        "\n",
        "                        # สร้างแผนภูมิเปรียบเทียบ\n",
        "                        print(\"\\nกำลังสร้างแผนภูมิเปรียบเทียบโมเดล...\")\n",
        "                        plot_model_comparison_comprehensive(results, prediction_intervals, siteName)\n",
        "\n",
        "                        # พิมพ์สรุปผล\n",
        "                        print_model_summary(results)\n",
        "\n",
        "                        # การสร้างกราฟเปรียบเทียบเดิม\n",
        "                        print(\"\\nกำลังสร้างกราฟ Actual vs Predicted...\")\n",
        "                        plot_actual_vs_predicted(\n",
        "                            y_test_rescaled,\n",
        "                            y_pred_rescaled,\n",
        "                            target_column='hotspot_count'\n",
        "                        )\n",
        "\n",
        "                    # การทำนาย 12 เดือนข้างหน้า (ปรับปรุง)\n",
        "                    print(\"\\n\" + \"=\"*60)\n",
        "                    print(\"กำลังทำนาย 12 เดือนข้างหน้า...\")\n",
        "                    print(\"=\"*60)\n",
        "\n",
        "                    last_sequence = X[-1:]\n",
        "                    predictions, prediction_dates = predict_next_12_months(\n",
        "                        model, last_sequence, scaler_x, scaler_y, df_cleaned, sequence_length=6\n",
        "                    )\n",
        "\n",
        "                    print(\"การทำนายเสร็จสมบูรณ์!\")\n",
        "\n",
        "                    # การสร้าง Prediction Intervals สำหรับการทำนาย 12 เดือน\n",
        "                    print(\"\\nกำลังคำนวดช่วงความเชื่อมั่นสำหรับการทำนาย...\")\n",
        "\n",
        "                    # คำนวด prediction intervals แบบง่าย (จาก residuals)\n",
        "                    if len(X_test) > 0:\n",
        "                        residuals = y_test_rescaled - y_pred_rescaled\n",
        "                        residual_std = np.std(residuals)\n",
        "\n",
        "                        # สร้าง confidence intervals\n",
        "                        confidence_80 = 1.282 * residual_std  # 80% CI\n",
        "                        confidence_95 = 1.96 * residual_std   # 95% CI\n",
        "\n",
        "                        prediction_intervals_12m = {\n",
        "                            '80%': {\n",
        "                                'lower': np.array(predictions) - confidence_80,\n",
        "                                'upper': np.array(predictions) + confidence_80\n",
        "                            },\n",
        "                            '95%': {\n",
        "                                'lower': np.array(predictions) - confidence_95,\n",
        "                                'upper': np.array(predictions) + confidence_95\n",
        "                            }\n",
        "                        }\n",
        "\n",
        "                        print(f\"ช่วงความเชื่อมั่น 80%: ±{confidence_80:.1f}\")\n",
        "                        print(f\"ช่วงความเชื่อมั่น 95%: ±{confidence_95:.1f}\")\n",
        "                    else:\n",
        "                        prediction_intervals_12m = {}\n",
        "\n",
        "                    # สร้างตารางสรุป\n",
        "                    print(\"\\nกำลังสร้างตารางสรุปการทำนาย...\")\n",
        "                    pred_summary_df = create_prediction_summary_table(\n",
        "                        predictions, prediction_dates, target_column='hotspot_count'\n",
        "                    )\n",
        "\n",
        "                    # ================================\n",
        "                    # การสร้างกราฟทำนาย 12 เดือน (ปรับปรุง)\n",
        "                    # ================================\n",
        "                    print(\"\\nกำลังสร้างกราฟการทำนาย 12 เดือน...\")\n",
        "                    plot_12_month_prediction(df_cleaned, predictions, prediction_dates, target_column='hotspot_count')\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"เกิดข้อผิดพลาดในการเตรียมข้อมูล: {e}\")\n",
        "                    import traceback\n",
        "                    print(\"รายละเอียดข้อผิดพลาด:\")\n",
        "                    traceback.print_exc()\n",
        "            else:\n",
        "                print(\"ข้อมูลไม่เพียงพอสำหรับการสร้างแบบจำลอง (ต้องการอย่างน้อย 24 เดือน)\")\n",
        "        else:\n",
        "            print(\"ไม่สามารถสร้างชุดข้อมูลได้\")\n",
        "    else:\n",
        "        print(\"ไม่สามารถดึงข้อมูล NDVI หรือ Hotspot ได้\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3ca88c",
      "metadata": {
        "id": "1b3ca88c"
      },
      "outputs": [],
      "source": [
        "# Get new data from GEE\n",
        "new_ndvi_data = get_monthly_ndvi('2025-01-01', '2025-12-31', study_area)\n",
        "new_hotspot_data = get_monthly_hotspots('2025-01-01', '2025-12-31', study_area)\n",
        "new_dataset = create_monthly_dataset_with_filtered_hotspots(new_ndvi_data, new_hotspot_data, study_area)\n",
        "new_df = fc_to_df(new_dataset)\n",
        "\n",
        "# Use saved model\n",
        "predictions, dates = load_and_predict(new_df, \"Chiang Mai_hotspot_model\", future_months=12)\n",
        "\n",
        "# Load model and predict on new data\n",
        "predictions, dates = load_and_predict(new_df, \"Chiang Mai_hotspot_model\", future_months=12)\n",
        "\n",
        "if predictions is not None:\n",
        "    # Create results\n",
        "    results_df = pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'predicted_hotspot_count': predictions\n",
        "    })\n",
        "    results_df.to_csv('new_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49109880",
      "metadata": {
        "id": "49109880"
      },
      "outputs": [],
      "source": [
        "# เริ่มต้นใช้งาน GEE\n",
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import calendar\n",
        "\n",
        "ee.Authenticate()\n",
        "try:\n",
        "    ee.Initialize(project=\"ee-sakda-451407\")\n",
        "except Exception as e:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project=\"ee-sakda-451407\")\n",
        "\n",
        "def save_prediction_to_csv(study_area, study_area_id, filename='predictions.csv'):\n",
        "    # Get data for the study area\n",
        "    new_ndvi_data = get_monthly_ndvi('2024-01-01', '2024-12-31', study_area)\n",
        "    new_hotspot_data = get_monthly_hotspots('2024-01-01', '2024-12-31', study_area)\n",
        "    new_dataset = create_monthly_dataset_with_filtered_hotspots(new_ndvi_data, new_hotspot_data, study_area)\n",
        "    new_df = fc_to_df(new_dataset)\n",
        "\n",
        "    # Load model and predict on new data\n",
        "    predictions, dates = load_and_predict(new_df, \"Chiang Mai_hotspot_model\", future_months=12)\n",
        "\n",
        "    if predictions is not None:\n",
        "        # Create results DataFrame with study area ID\n",
        "        results_df = pd.DataFrame({\n",
        "            'study_area_id': [study_area_id] * len(dates),  # Add ID column\n",
        "            'date': dates,\n",
        "            'predicted_hotspot_count': predictions\n",
        "        })\n",
        "\n",
        "        # Append to CSV (mode='a' for append, header only for first write)\n",
        "        mode = 'a' if study_area_id > 0 else 'w'  # Write header only for first area\n",
        "        header = True if study_area_id == 0 else False\n",
        "        results_df.to_csv(filename, mode=mode, header=header, index=False)\n",
        "\n",
        "# Feature collection\n",
        "feature = ee.FeatureCollection(\"projects/ee-sakda-451407/assets/fire/hex_forest_pro_4p_32647\")\n",
        "\n",
        "# Loop through study area IDs 1987\n",
        "for i in range(1987):\n",
        "    print(f\"Processing study area ID: {i}\")\n",
        "    new_study_area = feature.filter(ee.Filter.eq('id', i))\n",
        "    save_prediction_to_csv(new_study_area, study_area_id=i, filename='new_area_predictions.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}