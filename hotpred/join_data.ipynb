{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5366dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from shapely.geometry import shape\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_geojson(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load GeoJSON data from file with error handling.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to GeoJSON file\n",
    "        \n",
    "    Returns:\n",
    "        Parsed GeoJSON data as dictionary\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If file doesn't exist\n",
    "        json.JSONDecodeError: If file is not valid JSON\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"GeoJSON file not found: {file_path}\")\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_geojson(data: Dict[str, Any], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save GeoJSON data to file.\n",
    "    \n",
    "    Args:\n",
    "        data: GeoJSON data dictionary\n",
    "        file_path: Output file path\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    logger.info(f\"Saved GeoJSON to {file_path}\")\n",
    "\n",
    "\n",
    "def join_csv_to_geojson(\n",
    "    csv_path: str, \n",
    "    geojson_path: str, \n",
    "    output_path: str,\n",
    "    id_column: str = 'study_area_id',\n",
    "    date_column: str = 'date',\n",
    "    value_column: str = 'predicted_hotspot_count'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Join CSV predictions to GeoJSON features based on ID matching.\n",
    "    \n",
    "    This function is optimized using pandas groupby and dictionary operations\n",
    "    instead of iterrows() for better performance.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to input CSV file\n",
    "        geojson_path: Path to input GeoJSON file\n",
    "        output_path: Path to output GeoJSON file\n",
    "        id_column: Column name for feature ID in CSV\n",
    "        date_column: Column name for date in CSV\n",
    "        value_column: Column name for prediction values in CSV\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load CSV data\n",
    "        logger.info(f\"Loading CSV from {csv_path}\")\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Strip whitespace from column names (common issue in CSV files)\n",
    "        csv_data.columns = csv_data.columns.str.strip()\n",
    "        logger.info(f\"CSV columns: {list(csv_data.columns)}\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = [id_column, date_column, value_column]\n",
    "        missing_cols = [col for col in required_cols if col not in csv_data.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in CSV: {missing_cols}\")\n",
    "        \n",
    "        # Convert date format from d/m/yyyy to yyyy-mm-dd\n",
    "        logger.info(f\"Converting date format from d/m/yyyy to yyyy-mm-dd\")\n",
    "        csv_data[date_column] = pd.to_datetime(\n",
    "            csv_data[date_column], \n",
    "            format='%d/%m/%Y',\n",
    "            errors='coerce'  # Convert invalid dates to NaT\n",
    "        ).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Remove rows with invalid dates\n",
    "        invalid_dates = csv_data[date_column].isna().sum()\n",
    "        if invalid_dates > 0:\n",
    "            logger.warning(f\"Found {invalid_dates} invalid dates, removing those rows\")\n",
    "            csv_data = csv_data.dropna(subset=[date_column])\n",
    "        \n",
    "        logger.info(f\"Sample dates after conversion: {csv_data[date_column].head(3).tolist()}\")\n",
    "        \n",
    "        # Load GeoJSON data\n",
    "        logger.info(f\"Loading GeoJSON from {geojson_path}\")\n",
    "        geojson_data = load_geojson(geojson_path)\n",
    "        \n",
    "        # OPTIMIZATION: Use groupby and apply instead of iterrows for much better performance\n",
    "        # Group by study_area_id and create list of predictions\n",
    "        csv_data[id_column] = csv_data[id_column].astype(int)\n",
    "        predictions_dict = (\n",
    "            csv_data\n",
    "            .groupby(id_column)\n",
    "            .apply(lambda x: x[[date_column, value_column]].to_dict('records'), include_groups=False)\n",
    "            .to_dict()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Processing {len(geojson_data['features'])} features\")\n",
    "        \n",
    "        # Add predictions to matching GeoJSON features\n",
    "        matched_count = 0\n",
    "        for feature in geojson_data['features']:\n",
    "            feature_id = int(feature['properties'].get('id', -1))\n",
    "            if feature_id in predictions_dict:\n",
    "                feature['properties']['predictions'] = predictions_dict[feature_id]\n",
    "                matched_count += 1\n",
    "        \n",
    "        logger.info(f\"Matched {matched_count} features with predictions\")\n",
    "        \n",
    "        # Save updated GeoJSON\n",
    "        save_geojson(geojson_data, output_path)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error joining data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def remove_duplicate_polygons(\n",
    "    geojson_path: str, \n",
    "    output_path: str,\n",
    "    keep_properties: bool = True\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Remove duplicate polygon geometries from GeoJSON.\n",
    "    \n",
    "    Uses WKT (Well-Known Text) representation for reliable geometry comparison.\n",
    "    \n",
    "    Args:\n",
    "        geojson_path: Path to input GeoJSON file\n",
    "        output_path: Path to output GeoJSON file\n",
    "        keep_properties: If True, keeps properties from first occurrence\n",
    "        \n",
    "    Returns:\n",
    "        Number of duplicates removed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load GeoJSON data\n",
    "        logger.info(f\"Loading GeoJSON from {geojson_path}\")\n",
    "        geojson_data = load_geojson(geojson_path)\n",
    "        \n",
    "        original_count = len(geojson_data['features'])\n",
    "        unique_features = []\n",
    "        seen_geometries = set()\n",
    "        \n",
    "        logger.info(f\"Processing {original_count} features for duplicates\")\n",
    "        \n",
    "        # OPTIMIZATION: Use WKT for more reliable geometry comparison\n",
    "        for feature in geojson_data['features']:\n",
    "            try:\n",
    "                # Convert to Shapely geometry and get WKT representation\n",
    "                geom = shape(feature['geometry'])\n",
    "                geom_wkt = geom.wkt\n",
    "                \n",
    "                # Check if geometry is unique\n",
    "                if geom_wkt not in seen_geometries:\n",
    "                    seen_geometries.add(geom_wkt)\n",
    "                    unique_features.append(feature)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Skipping invalid geometry: {e}\")\n",
    "                continue\n",
    "        \n",
    "        duplicates_removed = original_count - len(unique_features)\n",
    "        \n",
    "        # Update GeoJSON with unique features\n",
    "        geojson_data['features'] = unique_features\n",
    "        \n",
    "        # Save result\n",
    "        save_geojson(geojson_data, output_path)\n",
    "        \n",
    "        logger.info(f\"Removed {duplicates_removed} duplicate polygons\")\n",
    "        logger.info(f\"Remaining features: {len(unique_features)}\")\n",
    "        \n",
    "        return duplicates_removed\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error removing duplicates: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Join CSV predictions to GeoJSON\n",
    "        join_csv_to_geojson(\n",
    "            csv_path=\"new_area_predictions.csv\",\n",
    "            geojson_path=\"hex_forest_pro_4326.geojson\",\n",
    "            output_path=\"hex_forest_pro_4326_with_predictions.geojson\"\n",
    "        )\n",
    "        \n",
    "        # Remove duplicate polygons\n",
    "        remove_duplicate_polygons(\n",
    "            geojson_path=\"hex_forest_pro_4326_with_predictions.geojson\",\n",
    "            output_path=\"hex_forest_pro_4326_predict.geojson\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Processing completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udfire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
